{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dadd35dc7b61458eb4bd8bb710acc8ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18aea09c6efa4f399842ee1fa161b6e7",
              "IPY_MODEL_7a90de6b7200430696b0bf4f681ee245",
              "IPY_MODEL_3b5d2a5cef78409095f6b110d546c6c2"
            ],
            "layout": "IPY_MODEL_a1094ead57274969a3a9d3203e8ccb6e"
          }
        },
        "18aea09c6efa4f399842ee1fa161b6e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90d89bdc27e844bc8c68a1d57de00fe0",
            "placeholder": "​",
            "style": "IPY_MODEL_d80357e11985435c964bab44ed3bcf1a",
            "value": "Project Progress:  38%"
          }
        },
        "7a90de6b7200430696b0bf4f681ee245": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc5a273a21cc481ab015bc0588444b47",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d157d0932eef44ac8fe2277dfe0c7577",
            "value": 3
          }
        },
        "3b5d2a5cef78409095f6b110d546c6c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2cfc7081e957476eb8be9dbc005bcd51",
            "placeholder": "​",
            "style": "IPY_MODEL_78646bd37ff24bae86c633850b63c1ea",
            "value": " 3/8 [00:02&lt;00:04,  1.20it/s]"
          }
        },
        "a1094ead57274969a3a9d3203e8ccb6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90d89bdc27e844bc8c68a1d57de00fe0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d80357e11985435c964bab44ed3bcf1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc5a273a21cc481ab015bc0588444b47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d157d0932eef44ac8fe2277dfe0c7577": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2cfc7081e957476eb8be9dbc005bcd51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78646bd37ff24bae86c633850b63c1ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ef3GD1xXPiDv",
        "outputId": "682d4e10-95e3-4485-a785-947494b20b5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the insurance.csv to these directory"
      ],
      "metadata": {
        "id": "B3n3WTOKirkE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cvpdrai4yLI"
      },
      "outputs": [],
      "source": [
        "!pip install numpy>=1.20.0 pandas>=1.3.0 matplotlib>=3.4.0 seaborn>=0.11.0 scikit-learn>=1.0.0 tensorflow>=2.8.0 scikeras>=0.9.0 tqdm>=4.61.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Insurance Charges Prediction Project (COSC 202)\n",
        "#\n",
        "# This notebook presents a comprehensive analysis and modeling of insurance charges prediction\n",
        "# using neural networks. The goal is to predict insurance charges based on customer attributes\n",
        "# such as age, BMI, smoking status, and other factors.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import datetime\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Use scikeras instead of the deprecated keras.wrappers.scikit_learn\n",
        "try:\n",
        "    from scikeras.wrappers import KerasRegressor\n",
        "except ImportError:\n",
        "    # Fallback for older TensorFlow versions\n",
        "    try:\n",
        "        from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "    except ImportError:\n",
        "        print(\"Warning: Could not import KerasRegressor. Please install scikeras package with: pip install scikeras\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)"
      ],
      "metadata": {
        "id": "Sh5EXhK2fyLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 1. Data Exploration\n",
        "#\n",
        "# This section explores the dataset to understand its structure, statistical properties,\n",
        "# and identify potential issues (missing values, outliers).\n",
        "\n",
        "def explore_data(df):\n",
        "    \"\"\"Explore the dataset and provide key statistics\"\"\"\n",
        "    print(\"\\n===== DATA EXPLORATION =====\\n\")\n",
        "\n",
        "    print(\"Dataset Shape:\", df.shape)\n",
        "    print(\"\\nData Types:\")\n",
        "    print(df.dtypes)\n",
        "\n",
        "    print(\"\\nBasic Statistics:\")\n",
        "    print(df.describe().T)\n",
        "\n",
        "    print(\"\\nChecking for Missing Values:\")\n",
        "    print(df.isnull().sum())\n",
        "\n",
        "    # Check for categorical features\n",
        "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "    print(\"\\nCategorical Features:\")\n",
        "    for col in categorical_cols:\n",
        "        print(f\"\\n{col} value counts:\")\n",
        "        print(df[col].value_counts())\n",
        "\n",
        "    # Check for outliers in numerical features\n",
        "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "    print(\"\\nChecking for Outliers in Numerical Features:\")\n",
        "    for col in numerical_cols:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        outliers = df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)]\n",
        "        print(f\"{col}: {len(outliers)} outliers detected ({len(outliers)/len(df)*100:.2f}%)\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "P1DEHlbff4Sg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 2. Data Visualization\n",
        "#\n",
        "# This section creates visualizations to understand feature relationships and their impact\n",
        "# on insurance charges.\n",
        "\n",
        "def visualize_data(df):\n",
        "    \"\"\"Visualize key patterns, correlations and distributions within dataset\"\"\"\n",
        "    print(\"\\n===== DATA VISUALIZATION =====\\n\")\n",
        "\n",
        "    # Set figure aesthetics\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    sns.set(font_scale=1.2)\n",
        "\n",
        "    # 1. Distribution of target variable (charges)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(df['charges'], kde=True)\n",
        "    plt.title('Distribution of Insurance Charges')\n",
        "    plt.xlabel('Charges ($)')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.boxplot(y=df['charges'])\n",
        "    plt.title('Boxplot of Insurance Charges')\n",
        "    plt.ylabel('Charges ($)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('charges_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 2. Correlation matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    # Convert categorical variables to numeric for correlation\n",
        "    df_corr = df.copy()\n",
        "    df_corr['sex'] = df_corr['sex'].map({'female': 0, 'male': 1})\n",
        "    df_corr['smoker'] = df_corr['smoker'].map({'no': 0, 'yes': 1})\n",
        "    df_corr = pd.get_dummies(df_corr, columns=['region'], drop_first=True)\n",
        "\n",
        "    corr = df_corr.corr()\n",
        "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "    sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', square=True)\n",
        "    plt.title('Correlation Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('correlation_matrix.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 3. Relationship between numerical features and charges\n",
        "    numerical_cols = ['age', 'bmi', 'children']\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, col in enumerate(numerical_cols):\n",
        "        plt.subplot(1, 3, i+1)\n",
        "        sns.scatterplot(x=col, y='charges', data=df, hue='smoker', alpha=0.7)\n",
        "        plt.title(f'{col.capitalize()} vs Charges')\n",
        "        plt.tight_layout()\n",
        "    plt.savefig('numerical_vs_charges.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 4. Relationship between categorical features and charges\n",
        "    categorical_cols = ['sex', 'smoker', 'region']\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, col in enumerate(categorical_cols):\n",
        "        plt.subplot(1, 3, i+1)\n",
        "        sns.boxplot(x=col, y='charges', data=df)\n",
        "        plt.title(f'{col.capitalize()} vs Charges')\n",
        "        plt.tight_layout()\n",
        "    plt.savefig('categorical_vs_charges.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 5. BMI distribution by smoker status\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.kdeplot(data=df, x='bmi', hue='smoker', fill=True, common_norm=False)\n",
        "    plt.title('BMI Distribution by Smoker Status')\n",
        "    plt.xlabel('BMI')\n",
        "    plt.ylabel('Density')\n",
        "    plt.savefig('bmi_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 6. Age vs Charges with BMI as size and smoker as hue\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.scatterplot(x='age', y='charges', size='bmi', hue='smoker', data=df, sizes=(20, 200), alpha=0.7)\n",
        "    plt.title('Age vs Charges (Size: BMI, Color: Smoker Status)')\n",
        "    plt.xlabel('Age')\n",
        "    plt.ylabel('Charges ($)')\n",
        "    plt.savefig('age_charges_bmi_smoker.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Print visualization conclusion\n",
        "    print(\"\\nVisualization Conclusions:\")\n",
        "    print(\"1. The charges distribution is right-skewed, indicating most people have lower charges but a few have very high charges.\")\n",
        "    print(\"2. Smoking status shows the strongest correlation with charges, indicating smokers are charged significantly more.\")\n",
        "    print(\"3. Age shows a positive correlation with charges - as age increases, charges tend to increase.\")\n",
        "    print(\"4. BMI has a moderate positive correlation with charges, especially for smokers.\")\n",
        "    print(\"5. There appears to be an interaction effect between smoking and BMI - higher BMI smokers face the highest charges.\")\n",
        "    print(\"6. Region and sex have less impact on charges compared to smoking status, age, and BMI.\")\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "WqAhXyJSf8vI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 3. Data Preprocessing\n",
        "#\n",
        "# This section handles data preprocessing including feature scaling, encoding categorical variables,\n",
        "# and splitting data into training and testing sets.\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Preprocess the dataset for neural network modeling\"\"\"\n",
        "    print(\"\\n===== DATA PREPROCESSING =====\\n\")\n",
        "\n",
        "    # Separate features and target\n",
        "    X = df.drop('charges', axis=1)\n",
        "    y = df['charges']\n",
        "\n",
        "    # Identify categorical and numerical columns\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Save the original test data for later analysis\n",
        "    X_test_original = X_test.copy()\n",
        "\n",
        "    # Create transformers for preprocessing\n",
        "    # Handle different versions of scikit-learn (sparse vs sparse_output parameter)\n",
        "    try:\n",
        "        # For scikit-learn 1.2+\n",
        "        categorical_transformer = OneHotEncoder(drop='first', sparse_output=False)\n",
        "    except TypeError:\n",
        "        # For older scikit-learn versions\n",
        "        categorical_transformer = OneHotEncoder(drop='first', sparse=False)\n",
        "\n",
        "    numerical_transformer = StandardScaler()\n",
        "\n",
        "    # Create a preprocessing pipeline using ColumnTransformer\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_cols),\n",
        "            ('cat', categorical_transformer, categorical_cols)\n",
        "        ])\n",
        "\n",
        "    # Fit the preprocessor on the training data\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "    # Print preprocessing results\n",
        "    print(f\"Training set shape: {X_train_processed.shape}\")\n",
        "    print(f\"Testing set shape: {X_test_processed.shape}\")\n",
        "\n",
        "    print(\"\\nPreprocessing Steps Applied:\")\n",
        "    print(\"1. One-hot encoded categorical variables (sex, smoker, region) with drop='first' to avoid dummy variable trap\")\n",
        "    print(\"2. Standardized numerical features (age, bmi, children) to have zero mean and unit variance\")\n",
        "    print(\"3. Split data into 80% training and 20% testing sets\")\n",
        "\n",
        "    # Return the processed datasets, the preprocessor, and original test data\n",
        "    return X_train_processed, X_test_processed, y_train, y_test, preprocessor, X_test_original\n"
      ],
      "metadata": {
        "id": "XjlcP_vOf_ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 4. Neural Network Model Design\n",
        "#\n",
        "# This section defines the neural network architecture for predicting insurance charges.\n",
        "\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "def create_model(input_dim, hidden_layers=2, neurons=64, dropout_rate=0.2, learning_rate=0.001):\n",
        "    \"\"\"Create a neural network model for regression\"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input layer using Input(shape) to avoid UserWarning\n",
        "    model.add(Input(shape=(input_dim,)))\n",
        "    model.add(Dense(neurons, activation='relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Hidden layers\n",
        "    for _ in range(hidden_layers - 1):\n",
        "        model.add(Dense(neurons, activation='relu'))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Output layer (linear activation for regression)\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='mse',\n",
        "        metrics=['mae'] # Keep MAE as a relevant regression metric\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "6cem44tlgC32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 5. Hyperparameter Tuning\n",
        "#\n",
        "# This section performs grid search with cross-validation to find optimal model hyperparameters.\n",
        "\n",
        "def tune_hyperparameters(X_train, y_train):\n",
        "    \"\"\"Tune neural network hyperparameters using GridSearchCV\"\"\"\n",
        "    print(\"\\n===== HYPERPARAMETER TUNING =====\\n\")\n",
        "\n",
        "    # Define a simplified model function that works with both keras and scikeras wrappers\n",
        "    def create_model_simple(input_dim=X_train.shape[1], hidden_layers=2, neurons=64, dropout_rate=0.2, learning_rate=0.001):\n",
        "        model = Sequential()\n",
        "\n",
        "        # Input layer\n",
        "        model.add(Dense(neurons, input_dim=input_dim, activation='relu'))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "\n",
        "        # Hidden layers\n",
        "        for _ in range(hidden_layers - 1):\n",
        "            model.add(Dense(neurons, activation='relu'))\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "        # Output layer (linear activation for regression)\n",
        "        model.add(Dense(1, activation='linear'))\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=learning_rate),\n",
        "            loss='mse',\n",
        "            metrics=['mae']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    # Create KerasRegressor wrapper - handle both legacy and scikeras versions\n",
        "    try:\n",
        "        # Check if we're using scikeras version\n",
        "        from scikeras.wrappers import KerasRegressor as ScikKerasRegressor\n",
        "        from sklearn.base import BaseEstimator, RegressorMixin\n",
        "\n",
        "        print(\"Using scikeras.wrappers.KerasRegressor with BaseEstimator and RegressorMixin\")\n",
        "\n",
        "        # Inherit from BaseEstimator and RegressorMixin to ensure compatibility with scikit-learn\n",
        "        class CompatibleKerasRegressor(ScikKerasRegressor, BaseEstimator, RegressorMixin):\n",
        "             def __sklearn_is_fitted__(self):\n",
        "                 # This method helps scikit-learn determine if the estimator is fitted\n",
        "                 return hasattr(self, '_estimator') and self._estimator is not None\n",
        "\n",
        "        # For scikeras, parameters are passed differently\n",
        "        model = CompatibleKerasRegressor(\n",
        "            model=create_model_simple,\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Define parameter grid for scikeras format\n",
        "        param_grid = {\n",
        "            'model__hidden_layers': [1, 2, 3],\n",
        "            'model__neurons': [32, 64, 128],\n",
        "            'model__dropout_rate': [0.1, 0.2, 0.3],\n",
        "            'model__learning_rate': [0.01, 0.001, 0.0001],\n",
        "            'batch_size': [16, 32, 64],\n",
        "            'epochs': [50, 100]\n",
        "        }\n",
        "    except ImportError:\n",
        "        # Legacy TensorFlow implementation\n",
        "        print(\"Using legacy tensorflow.keras.wrappers.scikit_learn.KerasRegressor\")\n",
        "        from tensorflow.keras.wrappers.scikit_learn import KerasRegressor as TFKerasRegressor\n",
        "\n",
        "        model = TFKerasRegressor(\n",
        "            build_fn=create_model_simple,\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Legacy parameter grid\n",
        "        param_grid = {\n",
        "            'hidden_layers': [1, 2, 3],\n",
        "            'neurons': [32, 64, 128],\n",
        "            'dropout_rate': [0.1, 0.2, 0.3],\n",
        "            'learning_rate': [0.01, 0.001, 0.0001],\n",
        "            'batch_size': [16, 32, 64],\n",
        "            'epochs': [50, 100]\n",
        "        }\n",
        "\n",
        "    # Define K-Fold cross-validation\n",
        "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # Create GridSearchCV\n",
        "    grid = GridSearchCV(\n",
        "        estimator=model,\n",
        "        param_grid=param_grid,\n",
        "        cv=kfold,\n",
        "        scoring='neg_mean_squared_error',\n",
        "        n_jobs=-1  # Use all available cores\n",
        "    )\n",
        "\n",
        "    print(\"Starting hyperparameter tuning with 5-fold cross-validation...\")\n",
        "    print(\"This may take some time to complete.\")\n",
        "\n",
        "    # Calculate total combinations for progress reporting\n",
        "    total_combinations = len(param_grid['batch_size']) * len(param_grid['epochs'])\n",
        "\n",
        "    # Get hidden_layers param appropriately whether using scikeras or legacy\n",
        "    if 'model__hidden_layers' in param_grid:\n",
        "        total_combinations *= len(param_grid['model__hidden_layers'])\n",
        "        total_combinations *= len(param_grid['model__neurons'])\n",
        "        total_combinations *= len(param_grid['model__dropout_rate'])\n",
        "        total_combinations *= len(param_grid['model__learning_rate'])\n",
        "    else:\n",
        "        total_combinations *= len(param_grid['hidden_layers'])\n",
        "        total_combinations *= len(param_grid['neurons'])\n",
        "        total_combinations *= len(param_grid['dropout_rate'])\n",
        "        total_combinations *= len(param_grid['learning_rate'])\n",
        "\n",
        "    # Calculate total fits (combinations * CV folds)\n",
        "    total_fits = total_combinations * kfold.get_n_splits()\n",
        "\n",
        "    print(f\"\\nPerforming {total_combinations} parameter combinations with {kfold.get_n_splits()}-fold cross-validation\")\n",
        "    print(f\"Total number of model fits: {total_fits}\")\n",
        "    print(\"This process may take a while. Progress updates will be shown periodically.\")\n",
        "\n",
        "    # Set verbose to 1 to get some progress information from GridSearchCV\n",
        "    grid.verbose = 1\n",
        "\n",
        "    # Fit GridSearchCV\n",
        "    print(\"\\nStarting hyperparameter search...\")\n",
        "    start_time = time.time()\n",
        "    grid_result = grid.fit(X_train, y_train)\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Print timing information\n",
        "    hours, remainder = divmod(elapsed_time, 3600)\n",
        "    minutes, seconds = divmod(remainder, 60)\n",
        "    print(f\"\\nHyperparameter search completed in {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nBest: {-grid_result.best_score_:.2f} MSE using {grid_result.best_params_}\")\n",
        "\n",
        "    # Print all results\n",
        "    print(\"\\nGrid Search Results:\")\n",
        "    means = -grid_result.cv_results_['mean_test_score']\n",
        "    stds = grid_result.cv_results_['std_test_score']\n",
        "    params = grid_result.cv_results_['params']\n",
        "\n",
        "    # Display top 5 results\n",
        "    top_indices = means.argsort()[:5]\n",
        "    print(\"\\nTop 5 hyperparameter combinations:\")\n",
        "    for i in top_indices:\n",
        "        print(f\"MSE: {means[i]:.2f} (+/- {stds[i]:.2f}) with: {params[i]}\")\n",
        "\n",
        "    # Return best parameters in standardized format (without model__prefix)\n",
        "    best_params = grid_result.best_params_.copy()\n",
        "\n",
        "    # For scikeras, convert model__param to param for consistency\n",
        "    if 'model__hidden_layers' in best_params:\n",
        "        best_params = {\n",
        "            'hidden_layers': best_params.get('model__hidden_layers', 2),\n",
        "            'neurons': best_params.get('model__neurons', 64),\n",
        "            'dropout_rate': best_params.get('model__dropout_rate', 0.2),\n",
        "            'learning_rate': best_params.get('model__learning_rate', 0.001),\n",
        "            'batch_size': best_params.get('batch_size', 32),\n",
        "            'epochs': best_params.get('epochs', 100)\n",
        "        }\n",
        "\n",
        "    return best_params\n",
        "\n",
        "# Hyperparameter Tuning (Manual Implementation)\n",
        "#\n",
        "# This section performs a manual grid search with cross-validation to find optimal model hyperparameters,\n",
        "# bypassing potential compatibility issues with Keras wrappers.\n",
        "\n",
        "def tune_hyperparameters_manual(X_train, y_train):\n",
        "    \"\"\"Tune neural network hyperparameters using a manual GridSearchCV implementation\"\"\"\n",
        "    print(\"\\n===== MANUAL HYPERPARAMETER TUNING =====\\n\")\n",
        "\n",
        "    # Define the parameter grid\n",
        "    param_grid = {\n",
        "        'hidden_layers': [1, 2, 3],\n",
        "        'neurons': [32, 64, 128],\n",
        "        'dropout_rate': [0.1, 0.2, 0.3],\n",
        "        'learning_rate': [0.01, 0.001, 0.0001],\n",
        "        'batch_size': [16, 32, 64],\n",
        "        'epochs': [50, 100]\n",
        "    }\n",
        "\n",
        "    # Define K-Fold cross-validation\n",
        "    n_splits = 5\n",
        "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    best_mse = float('inf')\n",
        "    best_params = {}\n",
        "    results = []\n",
        "\n",
        "    # Calculate total number of combinations for progress\n",
        "    total_combinations = 1\n",
        "    for key in param_grid:\n",
        "        total_combinations *= len(param_grid[key])\n",
        "\n",
        "    print(f\"Performing {total_combinations} parameter combinations with {n_splits}-fold cross-validation\")\n",
        "    print(f\"Total number of model fits: {total_combinations * n_splits}\")\n",
        "    print(\"This process will take a while. Progress will be updated per combination.\")\n",
        "\n",
        "    # Manual Grid Search\n",
        "    start_time = time.time()\n",
        "    combination_counter = 0\n",
        "\n",
        "    # Iterate through all hyperparameter combinations\n",
        "    for hidden_layers in param_grid['hidden_layers']:\n",
        "        for neurons in param_grid['neurons']:\n",
        "            for dropout_rate in param_grid['dropout_rate']:\n",
        "                for learning_rate in param_grid['learning_rate']:\n",
        "                    for batch_size in param_grid['batch_size']:\n",
        "                        for epochs in param_grid['epochs']:\n",
        "\n",
        "                            combination_counter += 1\n",
        "                            current_params = {\n",
        "                                'hidden_layers': hidden_layers,\n",
        "                                'neurons': neurons,\n",
        "                                'dropout_rate': dropout_rate,\n",
        "                                'learning_rate': learning_rate,\n",
        "                                'batch_size': batch_size,\n",
        "                                'epochs': epochs\n",
        "                            }\n",
        "                            print(f\"\\nEvaluating combination {combination_counter}/{total_combinations} with params: {current_params}\")\n",
        "\n",
        "                            fold_mse_scores = []\n",
        "\n",
        "                            # Perform K-Fold cross-validation\n",
        "                            for fold, (train_index, val_index) in enumerate(kfold.split(X_train, y_train)):\n",
        "                                print(f\"  Fold {fold+1}/{n_splits}...\")\n",
        "                                X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
        "                                y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
        "\n",
        "                                # Create and compile the model for the current fold\n",
        "                                model = create_model(\n",
        "                                    input_dim=X_train.shape[1],\n",
        "                                    hidden_layers=hidden_layers,\n",
        "                                    neurons=neurons,\n",
        "                                    dropout_rate=dropout_rate,\n",
        "                                    learning_rate=learning_rate\n",
        "                                )\n",
        "\n",
        "                                # Define callbacks for the fold\n",
        "                                early_stopping = EarlyStopping(\n",
        "                                    monitor='val_loss',\n",
        "                                    patience=10, # Reduced patience for faster tuning\n",
        "                                    restore_best_weights=True\n",
        "                                )\n",
        "\n",
        "                                # Train the model for the current fold\n",
        "                                history = model.fit(\n",
        "                                    X_train_fold,\n",
        "                                    y_train_fold,\n",
        "                                    validation_data=(X_val_fold, y_val_fold),\n",
        "                                    epochs=epochs,\n",
        "                                    batch_size=batch_size,\n",
        "                                    verbose=0, # Keep verbose low during tuning\n",
        "                                    callbacks=[early_stopping]\n",
        "                                )\n",
        "\n",
        "                                # Evaluate the model on the validation fold\n",
        "                                loss, mae = model.evaluate(X_val_fold, y_val_fold, verbose=0)\n",
        "                                fold_mse_scores.append(loss) # MSE is the first metric (loss)\n",
        "\n",
        "                            # Calculate the average MSE for the current combination across all folds\n",
        "                            mean_fold_mse = np.mean(fold_mse_scores)\n",
        "                            print(f\"  Average MSE for this combination: {mean_fold_mse:.2f}\")\n",
        "\n",
        "                            # Store results\n",
        "                            results.append({\n",
        "                                'params': current_params,\n",
        "                                'mean_mse': mean_fold_mse,\n",
        "                                'fold_mses': fold_mse_scores\n",
        "                            })\n",
        "\n",
        "                            # Update best parameters if current combination is better\n",
        "                            if mean_fold_mse < best_mse:\n",
        "                                best_mse = mean_fold_mse\n",
        "                                best_params = current_params.copy()\n",
        "\n",
        "    elapsed_time = time.time() - start_time\n",
        "    hours, remainder = divmod(elapsed_time, 3600)\n",
        "    minutes, seconds = divmod(remainder, 60)\n",
        "    print(f\"\\nManual hyperparameter search completed in {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")\n",
        "\n",
        "    print(f\"\\nBest parameters found: {best_params}\")\n",
        "    print(f\"Best mean cross-validation MSE: {best_mse:.2f}\")\n",
        "\n",
        "    # Optionally, sort and print top results\n",
        "    results.sort(key=lambda x: x['mean_mse'])\n",
        "    print(\"\\nTop 5 Hyperparameter Combinations:\")\n",
        "    for i in range(min(5, len(results))):\n",
        "        print(f\"  MSE: {results[i]['mean_mse']:.2f} with params: {results[i]['params']}\")\n",
        "\n",
        "\n",
        "    return best_params"
      ],
      "metadata": {
        "id": "-xIqxkkzgI-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 6. Model Training and Evaluation\n",
        "#\n",
        "# This section trains the final model with the best hyperparameters and evaluates performance.\n",
        "\n",
        "def train_and_evaluate(X_train, X_test, y_train, y_test, best_params, input_dim, X_test_original=None):\n",
        "    \"\"\"Train and evaluate the neural network model with best hyperparameters\"\"\"\n",
        "    print(\"\\n===== MODEL TRAINING AND EVALUATION =====\\n\")\n",
        "\n",
        "    # Extract best parameters\n",
        "    hidden_layers = best_params.get('hidden_layers', 2)\n",
        "    neurons = best_params.get('neurons', 64)\n",
        "    dropout_rate = best_params.get('dropout_rate', 0.2)\n",
        "    learning_rate = best_params.get('learning_rate', 0.001)\n",
        "    batch_size = best_params.get('batch_size', 32)\n",
        "    epochs = best_params.get('epochs', 100)\n",
        "\n",
        "    # Create model with best parameters\n",
        "    model = create_model(\n",
        "        input_dim=input_dim,\n",
        "        hidden_layers=hidden_layers,\n",
        "        neurons=neurons,\n",
        "        dropout_rate=dropout_rate,\n",
        "        learning_rate=learning_rate\n",
        "    )\n",
        "\n",
        "    # Create callbacks\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=20,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    model_checkpoint = ModelCheckpoint(\n",
        "        'best_model.h5',\n",
        "        save_best_only=True,\n",
        "        monitor='val_loss',\n",
        "        mode='min'\n",
        "    )\n",
        "\n",
        "    # Create progress bar callback for training\n",
        "    class TqdmProgressCallback(tf.keras.callbacks.Callback):\n",
        "        def on_epoch_begin(self, epoch, logs=None):\n",
        "            self.pbar = tqdm(total=int(self.params['steps']),\n",
        "                            desc=f\"Epoch {epoch+1}/{self.params['epochs']}\",\n",
        "                            position=0, leave=True)\n",
        "\n",
        "        def on_batch_end(self, batch, logs=None):\n",
        "            self.pbar.update(1)\n",
        "\n",
        "        def on_epoch_end(self, epoch, logs=None):\n",
        "            self.pbar.close()\n",
        "            train_loss = logs.get('loss', 0)\n",
        "            val_loss = logs.get('val_loss', 0)\n",
        "            train_mae = logs.get('mae', 0)\n",
        "            val_mae = logs.get('val_mae', 0)\n",
        "            print(f\"Epoch {epoch+1}/{self.params['epochs']} - loss: {train_loss:.4f} - val_loss: {val_loss:.4f} - mae: {train_mae:.4f} - val_mae: {val_mae:.4f}\")\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Training the final model with best hyperparameters...\")\n",
        "    history = model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        validation_split=0.2,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        verbose=0,  # Set to 0 since we're using our custom progress bar\n",
        "        callbacks=[early_stopping, model_checkpoint, TqdmProgressCallback()]\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on test data with a progress bar\n",
        "    print(\"Evaluating model on test data...\")\n",
        "    y_pred = np.zeros((len(X_test), 1))\n",
        "\n",
        "    # Use batch prediction with progress bar for large datasets\n",
        "    batch_size = 32\n",
        "    num_batches = int(np.ceil(len(X_test) / batch_size))\n",
        "\n",
        "    for i in tqdm(range(num_batches), desc=\"Predicting\"):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = min((i + 1) * batch_size, len(X_test))\n",
        "        y_pred[start_idx:end_idx] = model.predict(X_test[start_idx:end_idx], verbose=0)\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(\"\\nModel Performance on Test Data:\")\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "\n",
        "    # Enhanced visualization of training history\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot 1: Loss curves\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "    plt.title('Loss Curves', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Loss (MSE)', fontsize=12)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.legend(fontsize=10)\n",
        "\n",
        "    # Plot 2: MAE curves\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(history.history['mae'], label='Training MAE', color='green', linewidth=2)\n",
        "    plt.plot(history.history['val_mae'], label='Validation MAE', color='darkgreen', linewidth=2)\n",
        "    plt.title('Mean Absolute Error Curves', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('MAE', fontsize=12)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.legend(fontsize=10)\n",
        "\n",
        "    # Plot 3: Actual vs Predicted values\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.scatter(y_test, y_pred, alpha=0.6, edgecolor='k')\n",
        "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
        "    plt.xlabel('Actual Charges', fontsize=12)\n",
        "    plt.ylabel('Predicted Charges', fontsize=12)\n",
        "    plt.title('Actual vs Predicted Insurance Charges', fontsize=14)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Plot 4: Prediction Error Distribution\n",
        "    plt.subplot(2, 2, 4)\n",
        "    errors = y_test - y_pred.flatten()\n",
        "    sns.histplot(errors, kde=True)\n",
        "    plt.axvline(x=0, color='r', linestyle='--')\n",
        "    plt.title('Prediction Error Distribution', fontsize=14)\n",
        "    plt.xlabel('Prediction Error', fontsize=12)\n",
        "    plt.ylabel('Frequency', fontsize=12)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_evaluation.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # Additional plots for model analysis\n",
        "    plt.figure(figsize=(15, 12))\n",
        "\n",
        "    # Plot 1: Residuals vs Predicted Values\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.scatter(y_pred, errors, alpha=0.6, edgecolor='k')\n",
        "    plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "    plt.title('Residuals vs Predicted Values', fontsize=14)\n",
        "    plt.xlabel('Predicted Values', fontsize=12)\n",
        "    plt.ylabel('Residuals', fontsize=12)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Plot 2: Prediction Error vs Age\n",
        "    if 'age' in X_test_original.columns:\n",
        "        plt.subplot(2, 2, 2)\n",
        "        plt.scatter(X_test_original['age'], errors, alpha=0.6, edgecolor='k')\n",
        "        plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "        plt.title('Prediction Error vs Age', fontsize=14)\n",
        "        plt.xlabel('Age', fontsize=12)\n",
        "        plt.ylabel('Prediction Error', fontsize=12)\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Plot 3: Prediction Error vs BMI\n",
        "    if 'bmi' in X_test_original.columns:\n",
        "        plt.subplot(2, 2, 3)\n",
        "        plt.scatter(X_test_original['bmi'], errors, alpha=0.6, edgecolor='k')\n",
        "        plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "        plt.title('Prediction Error vs BMI', fontsize=14)\n",
        "        plt.xlabel('BMI', fontsize=12)\n",
        "        plt.ylabel('Prediction Error', fontsize=12)\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Plot 4: Error Distribution by Smoker Status\n",
        "    if 'smoker' in X_test_original.columns:\n",
        "        plt.subplot(2, 2, 4)\n",
        "        sns.boxplot(x=X_test_original['smoker'], y=errors)\n",
        "        plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "        plt.title('Prediction Error by Smoker Status', fontsize=14)\n",
        "        plt.xlabel('Smoker', fontsize=12)\n",
        "        plt.ylabel('Prediction Error', fontsize=12)\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_analysis.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "    return model, history, (mse, rmse, mae, r2)\n"
      ],
      "metadata": {
        "id": "WAGZTXRugPGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## 7. Model Saving and Persistence\n",
        "#\n",
        "# This section implements functionality to save all model artifacts and reload them later.\n",
        "\n",
        "def save_model_artifacts(model, preprocessor, history, metrics, best_params, output_dir='model_artifacts'):\n",
        "    \"\"\"\n",
        "    Save all model artifacts including the trained model, preprocessor, history, metrics, and parameters.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : tensorflow.keras.Model\n",
        "        The trained neural network model\n",
        "    preprocessor : sklearn.compose.ColumnTransformer\n",
        "        The preprocessing pipeline used for feature transformation\n",
        "    history : tensorflow.keras.callbacks.History\n",
        "        Training history object containing loss and metrics\n",
        "    metrics : tuple\n",
        "        Tuple containing (mse, rmse, mae, r2) performance metrics\n",
        "    best_params : dict\n",
        "        Dictionary of best hyperparameters\n",
        "    output_dir : str, default='model_artifacts'\n",
        "        Directory to save model artifacts\n",
        "    \"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Generate timestamp for versioning\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Define the artifacts to save\n",
        "    artifacts = [\n",
        "        \"Model\",\n",
        "        \"Preprocessor\",\n",
        "        \"Training History\",\n",
        "        \"Performance Metrics\",\n",
        "        \"Hyperparameters\",\n",
        "        \"Manifest\"\n",
        "    ]\n",
        "\n",
        "    # Create progress bar for saving artifacts\n",
        "    with tqdm(total=len(artifacts), desc=\"Saving Model Artifacts\") as saving_pbar:\n",
        "        # 1. Save the trained model\n",
        "        model_path = os.path.join(output_dir, f\"insurance_model_{timestamp}.h5\")\n",
        "        model.save(model_path)\n",
        "        saving_pbar.update(1)\n",
        "        saving_pbar.set_description(f\"Saved {artifacts[0]}\")\n",
        "\n",
        "        # 2. Save the preprocessor\n",
        "        preprocessor_path = os.path.join(output_dir, f\"preprocessor_{timestamp}.pkl\")\n",
        "        with open(preprocessor_path, 'wb') as f:\n",
        "            pickle.dump(preprocessor, f)\n",
        "        saving_pbar.update(1)\n",
        "        saving_pbar.set_description(f\"Saved {artifacts[1]}\")\n",
        "\n",
        "        # 3. Save training history\n",
        "        history_path = os.path.join(output_dir, f\"training_history_{timestamp}.json\")\n",
        "        with open(history_path, 'w') as f:\n",
        "            history_dict = {key: [float(val) for val in values] for key, values in history.history.items()}\n",
        "            json.dump(history_dict, f)\n",
        "        saving_pbar.update(1)\n",
        "        saving_pbar.set_description(f\"Saved {artifacts[2]}\")\n",
        "\n",
        "        # 4. Save performance metrics\n",
        "        metrics_path = os.path.join(output_dir, f\"metrics_{timestamp}.json\")\n",
        "        mse, rmse, mae, r2 = metrics\n",
        "        metrics_dict = {\n",
        "            'mse': float(mse),\n",
        "            'rmse': float(rmse),\n",
        "            'mae': float(mae),\n",
        "            'r2': float(r2)\n",
        "        }\n",
        "        with open(metrics_path, 'w') as f:\n",
        "            json.dump(metrics_dict, f)\n",
        "        saving_pbar.update(1)\n",
        "        saving_pbar.set_description(f\"Saved {artifacts[3]}\")\n",
        "\n",
        "        # 5. Save hyperparameters\n",
        "        params_path = os.path.join(output_dir, f\"hyperparameters_{timestamp}.json\")\n",
        "        with open(params_path, 'w') as f:\n",
        "            json.dump(best_params, f)\n",
        "        saving_pbar.update(1)\n",
        "        saving_pbar.set_description(f\"Saved {artifacts[4]}\")\n",
        "\n",
        "        # 6. Save a manifest file with paths to all artifacts\n",
        "        manifest_path = os.path.join(output_dir, f\"manifest_{timestamp}.json\")\n",
        "        manifest = {\n",
        "            'model_path': model_path,\n",
        "            'preprocessor_path': preprocessor_path,\n",
        "            'history_path': history_path,\n",
        "            'metrics_path': metrics_path,\n",
        "            'params_path': params_path,\n",
        "            'timestamp': timestamp\n",
        "        }\n",
        "        with open(manifest_path, 'w') as f:\n",
        "            json.dump(manifest, f)\n",
        "        saving_pbar.update(1)\n",
        "        saving_pbar.set_description(f\"Saved {artifacts[5]}\")\n",
        "\n",
        "    # Print summary of saved artifacts\n",
        "    print(f\"\\nAll model artifacts saved successfully with timestamp: {timestamp}\")\n",
        "    print(f\"  - Model: {os.path.basename(model_path)}\")\n",
        "    print(f\"  - Preprocessor: {os.path.basename(preprocessor_path)}\")\n",
        "    print(f\"  - History: {os.path.basename(history_path)}\")\n",
        "    print(f\"  - Metrics: {os.path.basename(metrics_path)}\")\n",
        "    print(f\"  - Parameters: {os.path.basename(params_path)}\")\n",
        "    print(f\"  - Manifest: {os.path.basename(manifest_path)}\")\n",
        "\n",
        "    return manifest\n",
        "\n",
        "\n",
        "def load_model_artifacts(manifest_path=None, timestamp=None, output_dir='model_artifacts'):\n",
        "    \"\"\"\n",
        "    Load model artifacts from saved files.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    manifest_path : str, optional\n",
        "        Path to the manifest file. If provided, will load artifacts specified in the manifest.\n",
        "    timestamp : str, optional\n",
        "        Timestamp of the artifacts to load. If provided instead of manifest_path, will try to load\n",
        "        artifacts with this timestamp.\n",
        "    output_dir : str, default='model_artifacts'\n",
        "        Directory where model artifacts are saved\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    model : tensorflow.keras.Model\n",
        "        The loaded model\n",
        "    preprocessor : sklearn.compose.ColumnTransformer\n",
        "        The loaded preprocessor\n",
        "    metrics : dict\n",
        "        Dictionary containing the performance metrics\n",
        "    params : dict\n",
        "        Dictionary containing the hyperparameters\n",
        "    \"\"\"\n",
        "    # If manifest path is provided, load from manifest\n",
        "    if manifest_path and os.path.exists(manifest_path):\n",
        "        with open(manifest_path, 'r') as f:\n",
        "            manifest = json.load(f)\n",
        "            model_path = manifest['model_path']\n",
        "            preprocessor_path = manifest['preprocessor_path']\n",
        "            metrics_path = manifest['metrics_path']\n",
        "            params_path = manifest['params_path']\n",
        "    # If timestamp is provided, construct paths\n",
        "    elif timestamp:\n",
        "        model_path = os.path.join(output_dir, f\"insurance_model_{timestamp}.h5\")\n",
        "        preprocessor_path = os.path.join(output_dir, f\"preprocessor_{timestamp}.pkl\")\n",
        "        metrics_path = os.path.join(output_dir, f\"metrics_{timestamp}.json\")\n",
        "        params_path = os.path.join(output_dir, f\"hyperparameters_{timestamp}.json\")\n",
        "    else:\n",
        "        # Find the most recent manifest file\n",
        "        manifest_files = [f for f in os.listdir(output_dir) if f.startswith('manifest_')]\n",
        "        if not manifest_files:\n",
        "            raise FileNotFoundError(\"No manifest files found in the output directory.\")\n",
        "\n",
        "        latest_manifest = sorted(manifest_files)[-1]\n",
        "        manifest_path = os.path.join(output_dir, latest_manifest)\n",
        "\n",
        "        with open(manifest_path, 'r') as f:\n",
        "            manifest = json.load(f)\n",
        "            model_path = manifest['model_path']\n",
        "            preprocessor_path = manifest['preprocessor_path']\n",
        "            metrics_path = manifest['metrics_path']\n",
        "            params_path = manifest['params_path']\n",
        "\n",
        "    # Load model\n",
        "    print(f\"Loading model from {model_path}...\")\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    # Load preprocessor\n",
        "    print(f\"Loading preprocessor from {preprocessor_path}...\")\n",
        "    with open(preprocessor_path, 'rb') as f:\n",
        "        preprocessor = pickle.load(f)\n",
        "\n",
        "    # Load metrics\n",
        "    print(f\"Loading metrics from {metrics_path}...\")\n",
        "    with open(metrics_path, 'r') as f:\n",
        "        metrics = json.load(f)\n",
        "\n",
        "    # Load hyperparameters\n",
        "    print(f\"Loading hyperparameters from {params_path}...\")\n",
        "    with open(params_path, 'r') as f:\n",
        "        params = json.load(f)\n",
        "\n",
        "    return model, preprocessor, metrics, params\n",
        "\n",
        "\n",
        "def make_prediction(model, preprocessor, new_data):\n",
        "    \"\"\"\n",
        "    Make predictions on new data using the loaded model and preprocessor.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : tensorflow.keras.Model\n",
        "        The trained model\n",
        "    preprocessor : sklearn.compose.ColumnTransformer\n",
        "        The preprocessing pipeline\n",
        "    new_data : pandas.DataFrame\n",
        "        New data to make predictions on\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    predictions : numpy.ndarray\n",
        "        Model predictions\n",
        "    \"\"\"\n",
        "    # Preprocess the new data\n",
        "    new_data_processed = preprocessor.transform(new_data)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(new_data_processed)\n",
        "\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "hLAvbVi7gVN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Show progress bar with project stages\n",
        "    project_steps = [\n",
        "        \"Loading dataset\",\n",
        "        \"Data exploration\",\n",
        "        \"Data visualization\",\n",
        "        \"Data preprocessing\",\n",
        "        \"Manual Hyperparameter tuning\", # Updated step name\n",
        "        \"Model training\",\n",
        "        \"Model evaluation\",\n",
        "        \"Saving model artifacts\"\n",
        "    ]\n",
        "\n",
        "    with tqdm(total=len(project_steps), desc=\"Project Progress\", position=0) as project_progress:\n",
        "        # Load the dataset\n",
        "        print(f\"\\n{project_steps[0]}...\")\n",
        "        df = pd.read_csv('insurance.csv')\n",
        "        project_progress.update(1)\n",
        "\n",
        "        # Data exploration\n",
        "        print(f\"\\n{project_steps[1]}...\")\n",
        "        df = explore_data(df)\n",
        "        project_progress.update(1)\n",
        "\n",
        "        # Data visualization\n",
        "        print(f\"\\n{project_steps[2]}...\")\n",
        "        df = visualize_data(df)\n",
        "        project_progress.update(1)\n",
        "\n",
        "        # Preprocess the data\n",
        "        print(f\"\\n{project_steps[3]}...\")\n",
        "        X_train_processed, X_test_processed, y_train, y_test, preprocessor, X_test_original = preprocess_data(df)\n",
        "        project_progress.update(1)\n",
        "\n",
        "        # Get number of input features after preprocessing\n",
        "        input_dim = X_train_processed.shape[1]\n",
        "\n",
        "        # Perform hyperparameter tuning using the manual implementation\n",
        "        print(f\"\\n{project_steps[4]}...\")\n",
        "        best_params = tune_hyperparameters_manual(X_train_processed, y_train) # Call the manual tuning function\n",
        "        project_progress.update(1)\n",
        "\n",
        "        # If you want to skip hyperparameter tuning, uncomment and use these parameters instead\n",
        "        # best_params = {\n",
        "        #     'hidden_layers': 2,\n",
        "        #     'neurons': 64,\n",
        "        #     'dropout_rate': 0.2,\n",
        "        #     'learning_rate': 0.001,\n",
        "        #     'batch_size': 32,\n",
        "        #     'epochs': 100\n",
        "        # }\n",
        "\n",
        "        # Train and evaluate the model\n",
        "        print(f\"\\n{project_steps[5]} and {project_steps[6]}...\")\n",
        "        model, history, metrics = train_and_evaluate(\n",
        "            X_train_processed, X_test_processed, y_train, y_test, best_params, input_dim, X_test_original)\n",
        "        project_progress.update(1)  # Count both training and evaluation as one step\n",
        "\n",
        "        # Save all model artifacts\n",
        "        print(\"\\nSaving model artifacts...\")\n",
        "        # Calculate total artifacts to save for the inner progress bar\n",
        "        num_artifacts_to_save = 5 # Model, Preprocessor, History, Metrics, Params, Manifest\n",
        "        with tqdm(total=num_artifacts_to_save, desc=\"Saving artifacts\") as saving_progress:\n",
        "             manifest = save_model_artifacts(model, preprocessor, history, metrics, best_params)\n",
        "             saving_progress.update(num_artifacts_to_save) # Update the inner progress bar to completion\n",
        "        project_progress.update(1) # Update the main project progress bar for saving artifacts\n",
        "\n",
        "    # Example of how to load the saved model and make predictions\n",
        "    print(\"\\n===== DEMONSTRATING MODEL LOADING =====\\n\")\n",
        "    loaded_model, loaded_preprocessor, loaded_metrics, loaded_params = load_model_artifacts(\n",
        "        manifest_path=manifest['manifest_path'])\n",
        "\n",
        "    # Create a small sample for prediction demonstration\n",
        "    sample = df.iloc[:5, :-1]  # First 5 rows, all columns except target\n",
        "    predictions = make_prediction(loaded_model, loaded_preprocessor, sample)\n",
        "\n",
        "    print(\"\\nSample Predictions:\")\n",
        "    for i, (idx, row) in enumerate(sample.iterrows()):\n",
        "        print(f\"Sample {i+1}: Age: {row['age']}, Sex: {row['sex']}, BMI: {row['bmi']:.1f}, \"\n",
        "              f\"Children: {row['children']}, Smoker: {row['smoker']}, Region: {row['region']}\")\n",
        "        print(f\"Predicted Charge: ${predictions[i][0]:.2f}, Actual Charge: ${df.iloc[i]['charges']:.2f}\\n\")\n",
        "\n",
        "    print(\"\\n===== PROJECT SUMMARY =====\\n\")\n",
        "    print(\"1. Explored the insurance dataset and identified key patterns\")\n",
        "    print(\"2. Visualized relationships between features and insurance charges\")\n",
        "    print(\"3. Preprocessed the data by encoding categorical variables and scaling numerical features\")\n",
        "    print(\"4. Performed hyperparameter tuning with cross-validation to find optimal neural network configuration\")\n",
        "    print(\"5. Trained final neural network model with early stopping to prevent overfitting\")\n",
        "    print(\"6. Evaluated model performance using multiple metrics\")\n",
        "    print(\"7. Saved all model artifacts for future use (model, preprocessor, metrics, hyperparameters)\")\n",
        "    print(\"8. Implemented functionality to load saved models and make new predictions\")\n",
        "    print(\"\\nThe model can now be used to predict insurance charges based on customer attributes.\")\n",
        "    print(\"All visualizations and model artifacts have been saved in the current directory.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "dadd35dc7b61458eb4bd8bb710acc8ca",
            "18aea09c6efa4f399842ee1fa161b6e7",
            "7a90de6b7200430696b0bf4f681ee245",
            "3b5d2a5cef78409095f6b110d546c6c2",
            "a1094ead57274969a3a9d3203e8ccb6e",
            "90d89bdc27e844bc8c68a1d57de00fe0",
            "d80357e11985435c964bab44ed3bcf1a",
            "fc5a273a21cc481ab015bc0588444b47",
            "d157d0932eef44ac8fe2277dfe0c7577",
            "2cfc7081e957476eb8be9dbc005bcd51",
            "78646bd37ff24bae86c633850b63c1ea"
          ]
        },
        "id": "2_fr7hWOP1DT",
        "outputId": "81ee4d5b-123f-4d36-c769-8d04fe07ad8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Project Progress:   0%|          | 0/8 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dadd35dc7b61458eb4bd8bb710acc8ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loading dataset...\n",
            "\n",
            "Data exploration...\n",
            "\n",
            "===== DATA EXPLORATION =====\n",
            "\n",
            "Dataset Shape: (1338, 7)\n",
            "\n",
            "Data Types:\n",
            "age           int64\n",
            "sex          object\n",
            "bmi         float64\n",
            "children      int64\n",
            "smoker       object\n",
            "region       object\n",
            "charges     float64\n",
            "dtype: object\n",
            "\n",
            "Basic Statistics:\n",
            "           count          mean           std        min         25%       50%  \\\n",
            "age       1338.0     39.207025     14.049960    18.0000    27.00000    39.000   \n",
            "bmi       1338.0     30.663397      6.098187    15.9600    26.29625    30.400   \n",
            "children  1338.0      1.094918      1.205493     0.0000     0.00000     1.000   \n",
            "charges   1338.0  13270.422265  12110.011237  1121.8739  4740.28715  9382.033   \n",
            "\n",
            "                   75%          max  \n",
            "age          51.000000     64.00000  \n",
            "bmi          34.693750     53.13000  \n",
            "children      2.000000      5.00000  \n",
            "charges   16639.912515  63770.42801  \n",
            "\n",
            "Checking for Missing Values:\n",
            "age         0\n",
            "sex         0\n",
            "bmi         0\n",
            "children    0\n",
            "smoker      0\n",
            "region      0\n",
            "charges     0\n",
            "dtype: int64\n",
            "\n",
            "Categorical Features:\n",
            "\n",
            "sex value counts:\n",
            "sex\n",
            "male      676\n",
            "female    662\n",
            "Name: count, dtype: int64\n",
            "\n",
            "smoker value counts:\n",
            "smoker\n",
            "no     1064\n",
            "yes     274\n",
            "Name: count, dtype: int64\n",
            "\n",
            "region value counts:\n",
            "region\n",
            "southeast    364\n",
            "southwest    325\n",
            "northwest    325\n",
            "northeast    324\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Checking for Outliers in Numerical Features:\n",
            "age: 0 outliers detected (0.00%)\n",
            "bmi: 9 outliers detected (0.67%)\n",
            "children: 0 outliers detected (0.00%)\n",
            "charges: 139 outliers detected (10.39%)\n",
            "\n",
            "Data visualization...\n",
            "\n",
            "===== DATA VISUALIZATION =====\n",
            "\n",
            "\n",
            "Visualization Conclusions:\n",
            "1. The charges distribution is right-skewed, indicating most people have lower charges but a few have very high charges.\n",
            "2. Smoking status shows the strongest correlation with charges, indicating smokers are charged significantly more.\n",
            "3. Age shows a positive correlation with charges - as age increases, charges tend to increase.\n",
            "4. BMI has a moderate positive correlation with charges, especially for smokers.\n",
            "5. There appears to be an interaction effect between smoking and BMI - higher BMI smokers face the highest charges.\n",
            "6. Region and sex have less impact on charges compared to smoking status, age, and BMI.\n",
            "\n",
            "Data preprocessing...\n",
            "\n",
            "===== DATA PREPROCESSING =====\n",
            "\n",
            "Training set shape: (1070, 8)\n",
            "Testing set shape: (268, 8)\n",
            "\n",
            "Preprocessing Steps Applied:\n",
            "1. One-hot encoded categorical variables (sex, smoker, region) with drop='first' to avoid dummy variable trap\n",
            "2. Standardized numerical features (age, bmi, children) to have zero mean and unit variance\n",
            "3. Split data into 80% training and 20% testing sets\n",
            "\n",
            "Manual Hyperparameter tuning...\n",
            "\n",
            "===== MANUAL HYPERPARAMETER TUNING =====\n",
            "\n",
            "Performing 486 parameter combinations with 5-fold cross-validation\n",
            "Total number of model fits: 2430\n",
            "This process will take a while. Progress will be updated per combination.\n",
            "\n",
            "Evaluating combination 1/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.1, 'learning_rate': 0.01, 'batch_size': 16, 'epochs': 50}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 64866691.20\n",
            "\n",
            "Evaluating combination 2/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.1, 'learning_rate': 0.01, 'batch_size': 16, 'epochs': 100}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 38623147.60\n",
            "\n",
            "Evaluating combination 3/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.1, 'learning_rate': 0.01, 'batch_size': 32, 'epochs': 50}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 99076913.60\n",
            "\n",
            "Evaluating combination 4/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.1, 'learning_rate': 0.01, 'batch_size': 32, 'epochs': 100}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 60284896.00\n",
            "\n",
            "Evaluating combination 5/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.1, 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 50}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 174647753.60\n",
            "\n",
            "Evaluating combination 6/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.1, 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 100}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 96792008.00\n",
            "\n",
            "Evaluating combination 7/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'batch_size': 16, 'epochs': 50}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 298701574.40\n",
            "\n",
            "Evaluating combination 8/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'batch_size': 16, 'epochs': 100}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 250946076.80\n",
            "\n",
            "Evaluating combination 9/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 50}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 315452384.00\n",
            "\n",
            "Evaluating combination 10/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 295702950.40\n",
            "\n",
            "Evaluating combination 11/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'batch_size': 64, 'epochs': 50}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 320491993.60\n",
            "\n",
            "Evaluating combination 12/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.1, 'learning_rate': 0.001, 'batch_size': 64, 'epochs': 100}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 314158259.20\n",
            "\n",
            "Evaluating combination 13/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.1, 'learning_rate': 0.0001, 'batch_size': 16, 'epochs': 50}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 322206937.60\n",
            "\n",
            "Evaluating combination 14/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.1, 'learning_rate': 0.0001, 'batch_size': 16, 'epochs': 100}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 321695545.60\n",
            "\n",
            "Evaluating combination 15/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.1, 'learning_rate': 0.0001, 'batch_size': 32, 'epochs': 50}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 322369126.40\n",
            "\n",
            "Evaluating combination 16/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.1, 'learning_rate': 0.0001, 'batch_size': 32, 'epochs': 100}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 322193171.20\n",
            "\n",
            "Evaluating combination 17/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.1, 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 50}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 322409766.40\n",
            "\n",
            "Evaluating combination 18/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.1, 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 100}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 322339276.80\n",
            "\n",
            "Evaluating combination 19/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'batch_size': 16, 'epochs': 50}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 66877511.20\n",
            "\n",
            "Evaluating combination 20/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'batch_size': 16, 'epochs': 100}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 38613316.80\n",
            "\n",
            "Evaluating combination 21/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'batch_size': 32, 'epochs': 50}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 95363660.80\n",
            "\n",
            "Evaluating combination 22/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'batch_size': 32, 'epochs': 100}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 63621932.80\n",
            "\n",
            "Evaluating combination 23/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 50}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 166631868.80\n",
            "\n",
            "Evaluating combination 24/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.2, 'learning_rate': 0.01, 'batch_size': 64, 'epochs': 100}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 94890283.20\n",
            "\n",
            "Evaluating combination 25/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 16, 'epochs': 50}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 298536579.20\n",
            "\n",
            "Evaluating combination 26/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 16, 'epochs': 100}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 253175721.60\n",
            "\n",
            "Evaluating combination 27/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 50}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 315262649.60\n",
            "\n",
            "Evaluating combination 28/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 296122035.20\n",
            "\n",
            "Evaluating combination 29/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 64, 'epochs': 50}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 320380710.40\n",
            "\n",
            "Evaluating combination 30/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 64, 'epochs': 100}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 313986028.80\n",
            "\n",
            "Evaluating combination 31/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.2, 'learning_rate': 0.0001, 'batch_size': 16, 'epochs': 50}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 322210924.80\n",
            "\n",
            "Evaluating combination 32/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.2, 'learning_rate': 0.0001, 'batch_size': 16, 'epochs': 100}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 321651692.80\n",
            "\n",
            "Evaluating combination 33/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.2, 'learning_rate': 0.0001, 'batch_size': 32, 'epochs': 50}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 322370860.80\n",
            "\n",
            "Evaluating combination 34/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.2, 'learning_rate': 0.0001, 'batch_size': 32, 'epochs': 100}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 322194508.80\n",
            "\n",
            "Evaluating combination 35/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.2, 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 50}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 322409849.60\n",
            "\n",
            "Evaluating combination 36/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.2, 'learning_rate': 0.0001, 'batch_size': 64, 'epochs': 100}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 322343033.60\n",
            "\n",
            "Evaluating combination 37/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'batch_size': 16, 'epochs': 50}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 69410206.40\n",
            "\n",
            "Evaluating combination 38/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'batch_size': 16, 'epochs': 100}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 39179140.80\n",
            "\n",
            "Evaluating combination 39/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'batch_size': 32, 'epochs': 50}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n",
            "  Average MSE for this combination: 97747318.40\n",
            "\n",
            "Evaluating combination 40/486 with params: {'hidden_layers': 1, 'neurons': 32, 'dropout_rate': 0.3, 'learning_rate': 0.01, 'batch_size': 32, 'epochs': 100}\n",
            "  Fold 1/5...\n",
            "  Fold 2/5...\n",
            "  Fold 3/5...\n",
            "  Fold 4/5...\n",
            "  Fold 5/5...\n"
          ]
        }
      ]
    }
  ]
}
