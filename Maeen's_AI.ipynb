{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7cvpdrai4yLI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: 1.20.0 not found\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy>=1.20.0 pandas>=1.3.0 matplotlib>=3.4.0 seaborn>=0.11.0 scikit-learn>=1.0.0 tensorflow>=2.8.0 scikeras>=0.9.0 tqdm>=4.61.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Sh5EXhK2fyLZ"
      },
      "outputs": [],
      "source": [
        "# # Insurance Charges Prediction Project (COSC 202)\n",
        "#\n",
        "# This notebook presents a comprehensive analysis and modeling of insurance charges prediction\n",
        "# using neural networks. The goal is to predict insurance charges based on customer attributes\n",
        "# such as age, BMI, smoking status, and other factors.\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import datetime\n",
        "import time\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Use scikeras instead of the deprecated keras.wrappers.scikit_learn\n",
        "try:\n",
        "    from scikeras.wrappers import KerasRegressor\n",
        "except ImportError:\n",
        "    # Fallback for older TensorFlow versions\n",
        "    try:\n",
        "        from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
        "    except ImportError:\n",
        "        print(\"Warning: Could not import KerasRegressor. Please install scikeras package with: pip install scikeras\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "P1DEHlbff4Sg"
      },
      "outputs": [],
      "source": [
        "# ## 1. Data Exploration\n",
        "#\n",
        "# This section explores the dataset to understand its structure, statistical properties,\n",
        "# and identify potential issues (missing values, outliers).\n",
        "\n",
        "def explore_data(df):\n",
        "    \"\"\"Explore the dataset and provide key statistics\"\"\"\n",
        "    print(\"\\n===== DATA EXPLORATION =====\\n\")\n",
        "\n",
        "    print(\"Dataset Shape:\", df.shape)\n",
        "    print(\"\\nData Types:\")\n",
        "    print(df.dtypes)\n",
        "\n",
        "    print(\"\\nBasic Statistics:\")\n",
        "    print(df.describe().T)\n",
        "\n",
        "    print(\"\\nChecking for Missing Values:\")\n",
        "    print(df.isnull().sum())\n",
        "\n",
        "    # Check for categorical features\n",
        "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "    print(\"\\nCategorical Features:\")\n",
        "    for col in categorical_cols:\n",
        "        print(f\"\\n{col} value counts:\")\n",
        "        print(df[col].value_counts())\n",
        "\n",
        "    # Check for outliers in numerical features\n",
        "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "    print(\"\\nChecking for Outliers in Numerical Features:\")\n",
        "    for col in numerical_cols:\n",
        "        Q1 = df[col].quantile(0.25)\n",
        "        Q3 = df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        outliers = df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)]\n",
        "        print(f\"{col}: {len(outliers)} outliers detected ({len(outliers)/len(df)*100:.2f}%)\")\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "WqAhXyJSf8vI"
      },
      "outputs": [],
      "source": [
        "# ## 2. Data Visualization\n",
        "#\n",
        "# This section creates visualizations to understand feature relationships and their impact\n",
        "# on insurance charges.\n",
        "\n",
        "def visualize_data(df):\n",
        "    \"\"\"Visualize key patterns, correlations and distributions within dataset\"\"\"\n",
        "    print(\"\\n===== DATA VISUALIZATION =====\\n\")\n",
        "\n",
        "    # Set figure aesthetics\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "    sns.set(font_scale=1.2)\n",
        "\n",
        "    # 1. Distribution of target variable (charges)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.histplot(df['charges'], kde=True)\n",
        "    plt.title('Distribution of Insurance Charges')\n",
        "    plt.xlabel('Charges ($)')\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.boxplot(y=df['charges'])\n",
        "    plt.title('Boxplot of Insurance Charges')\n",
        "    plt.ylabel('Charges ($)')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('charges_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 2. Correlation matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    # Convert categorical variables to numeric for correlation\n",
        "    df_corr = df.copy()\n",
        "    df_corr['sex'] = df_corr['sex'].map({'female': 0, 'male': 1})\n",
        "    df_corr['smoker'] = df_corr['smoker'].map({'no': 0, 'yes': 1})\n",
        "    df_corr = pd.get_dummies(df_corr, columns=['region'], drop_first=True)\n",
        "\n",
        "    corr = df_corr.corr()\n",
        "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "    sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='coolwarm', square=True)\n",
        "    plt.title('Correlation Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('correlation_matrix.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 3. Relationship between numerical features and charges\n",
        "    numerical_cols = ['age', 'bmi', 'children']\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, col in enumerate(numerical_cols):\n",
        "        plt.subplot(1, 3, i+1)\n",
        "        sns.scatterplot(x=col, y='charges', data=df, hue='smoker', alpha=0.7)\n",
        "        plt.title(f'{col.capitalize()} vs Charges')\n",
        "        plt.tight_layout()\n",
        "    plt.savefig('numerical_vs_charges.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 4. Relationship between categorical features and charges\n",
        "    categorical_cols = ['sex', 'smoker', 'region']\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, col in enumerate(categorical_cols):\n",
        "        plt.subplot(1, 3, i+1)\n",
        "        sns.boxplot(x=col, y='charges', data=df)\n",
        "        plt.title(f'{col.capitalize()} vs Charges')\n",
        "        plt.tight_layout()\n",
        "    plt.savefig('categorical_vs_charges.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 5. BMI distribution by smoker status\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.kdeplot(data=df, x='bmi', hue='smoker', fill=True, common_norm=False)\n",
        "    plt.title('BMI Distribution by Smoker Status')\n",
        "    plt.xlabel('BMI')\n",
        "    plt.ylabel('Density')\n",
        "    plt.savefig('bmi_distribution.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 6. Age vs Charges with BMI as size and smoker as hue\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    sns.scatterplot(x='age', y='charges', size='bmi', hue='smoker', data=df, sizes=(20, 200), alpha=0.7)\n",
        "    plt.title('Age vs Charges (Size: BMI, Color: Smoker Status)')\n",
        "    plt.xlabel('Age')\n",
        "    plt.ylabel('Charges ($)')\n",
        "    plt.savefig('age_charges_bmi_smoker.png')\n",
        "    plt.close()\n",
        "\n",
        "    # Print visualization conclusion\n",
        "    print(\"\\nVisualization Conclusions:\")\n",
        "    print(\"1. The charges distribution is right-skewed, indicating most people have lower charges but a few have very high charges.\")\n",
        "    print(\"2. Smoking status shows the strongest correlation with charges, indicating smokers are charged significantly more.\")\n",
        "    print(\"3. Age shows a positive correlation with charges - as age increases, charges tend to increase.\")\n",
        "    print(\"4. BMI has a moderate positive correlation with charges, especially for smokers.\")\n",
        "    print(\"5. There appears to be an interaction effect between smoking and BMI - higher BMI smokers face the highest charges.\")\n",
        "    print(\"6. Region and sex have less impact on charges compared to smoking status, age, and BMI.\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "XjlcP_vOf_ee"
      },
      "outputs": [],
      "source": [
        "# ## 3. Data Preprocessing\n",
        "#\n",
        "# This section handles data preprocessing including feature scaling, encoding categorical variables,\n",
        "# and splitting data into training and testing sets.\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Preprocess the dataset for neural network modeling\"\"\"\n",
        "    print(\"\\n===== DATA PREPROCESSING =====\\n\")\n",
        "\n",
        "    # Separate features and target\n",
        "    X = df.drop('charges', axis=1)\n",
        "    y = df['charges']\n",
        "\n",
        "    # Identify categorical and numerical columns\n",
        "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
        "    numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Save the original test data for later analysis\n",
        "    X_test_original = X_test.copy()\n",
        "\n",
        "    # Create transformers for preprocessing\n",
        "    # Handle different versions of scikit-learn (sparse vs sparse_output parameter)\n",
        "    try:\n",
        "        # For scikit-learn 1.2+\n",
        "        categorical_transformer = OneHotEncoder(drop='first', sparse_output=False)\n",
        "    except TypeError:\n",
        "        # For older scikit-learn versions\n",
        "        categorical_transformer = OneHotEncoder(drop='first', sparse=False)\n",
        "\n",
        "    numerical_transformer = StandardScaler()\n",
        "\n",
        "    # Create a preprocessing pipeline using ColumnTransformer\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_cols),\n",
        "            ('cat', categorical_transformer, categorical_cols)\n",
        "        ])\n",
        "\n",
        "    # Fit the preprocessor on the training data\n",
        "    X_train_processed = preprocessor.fit_transform(X_train)\n",
        "    X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "    # Print preprocessing results\n",
        "    print(f\"Training set shape: {X_train_processed.shape}\")\n",
        "    print(f\"Testing set shape: {X_test_processed.shape}\")\n",
        "\n",
        "    print(\"\\nPreprocessing Steps Applied:\")\n",
        "    print(\"1. One-hot encoded categorical variables (sex, smoker, region) with drop='first' to avoid dummy variable trap\")\n",
        "    print(\"2. Standardized numerical features (age, bmi, children) to have zero mean and unit variance\")\n",
        "    print(\"3. Split data into 80% training and 20% testing sets\")\n",
        "\n",
        "    # Return the processed datasets, the preprocessor, and original test data\n",
        "    return X_train_processed, X_test_processed, y_train, y_test, preprocessor, X_test_original\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "6cem44tlgC32"
      },
      "outputs": [],
      "source": [
        "# ## 4. Neural Network Model Design\n",
        "#\n",
        "# This section defines the neural network architecture for predicting insurance charges.\n",
        "\n",
        "def create_model(input_dim, hidden_layers=2, neurons=64, dropout_rate=0.2, learning_rate=0.001):\n",
        "    \"\"\"Create a neural network model for regression\"\"\"\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input layer\n",
        "    model.add(Dense(neurons, input_dim=input_dim, activation='relu'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Hidden layers\n",
        "    for _ in range(hidden_layers - 1):\n",
        "        model.add(Dense(neurons, activation='relu'))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "\n",
        "    # Output layer (linear activation for regression)\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=learning_rate),\n",
        "        loss='mse',\n",
        "        metrics=['mae']\n",
        "    )\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "-xIqxkkzgI-d"
      },
      "outputs": [],
      "source": [
        "# ## 5. Hyperparameter Tuning\n",
        "#\n",
        "# This section performs grid search with cross-validation to find optimal model hyperparameters.\n",
        "\n",
        "def tune_hyperparameters(X_train, y_train):\n",
        "    \"\"\"Tune neural network hyperparameters using GridSearchCV\"\"\"\n",
        "    print(\"\\n===== HYPERPARAMETER TUNING =====\\n\")\n",
        "\n",
        "    # Define a simplified model function that works with both keras and scikeras wrappers\n",
        "    def create_model_simple(input_dim=X_train.shape[1], hidden_layers=2, neurons=64, dropout_rate=0.2, learning_rate=0.001):\n",
        "        model = Sequential()\n",
        "\n",
        "        # Input layer\n",
        "        model.add(Dense(neurons, input_dim=input_dim, activation='relu'))\n",
        "        model.add(Dropout(dropout_rate))\n",
        "\n",
        "        # Hidden layers\n",
        "        for _ in range(hidden_layers - 1):\n",
        "            model.add(Dense(neurons, activation='relu'))\n",
        "            model.add(Dropout(dropout_rate))\n",
        "\n",
        "        # Output layer (linear activation for regression)\n",
        "        model.add(Dense(1, activation='linear'))\n",
        "\n",
        "        # Compile the model\n",
        "        model.compile(\n",
        "            optimizer=Adam(learning_rate=learning_rate),\n",
        "            loss='mse',\n",
        "            metrics=['mae']\n",
        "        )\n",
        "\n",
        "        return model\n",
        "\n",
        "    # Create KerasRegressor wrapper - handle both legacy and scikeras versions\n",
        "    try:\n",
        "        # Check if we're using scikeras version\n",
        "        from scikeras.wrappers import KerasRegressor as ScikKerasRegressor\n",
        "        print(\"Using scikeras.wrappers.KerasRegressor\")\n",
        "\n",
        "        # For scikeras, parameters are passed differently\n",
        "        model = ScikKerasRegressor(\n",
        "            model=create_model_simple,\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Define parameter grid for scikeras format\n",
        "        param_grid = {\n",
        "            'model__hidden_layers': [1, 2, 3],\n",
        "            'model__neurons': [32, 64, 128],\n",
        "            'model__dropout_rate': [0.1, 0.2, 0.3],\n",
        "            'model__learning_rate': [0.01, 0.001, 0.0001],\n",
        "            'batch_size': [16, 32, 64],\n",
        "            'epochs': [50, 100]\n",
        "        }\n",
        "    except ImportError:\n",
        "        # Legacy TensorFlow implementation\n",
        "        print(\"Using legacy tensorflow.keras.wrappers.scikit_learn.KerasRegressor\")\n",
        "        from tensorflow.keras.wrappers.scikit_learn import KerasRegressor as TFKerasRegressor\n",
        "\n",
        "        model = TFKerasRegressor(\n",
        "            build_fn=create_model_simple,\n",
        "            epochs=100,\n",
        "            batch_size=32,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Legacy parameter grid\n",
        "        param_grid = {\n",
        "            'hidden_layers': [1, 2, 3],\n",
        "            'neurons': [32, 64, 128],\n",
        "            'dropout_rate': [0.1, 0.2, 0.3],\n",
        "            'learning_rate': [0.01, 0.001, 0.0001],\n",
        "            'batch_size': [16, 32, 64],\n",
        "            'epochs': [50, 100]\n",
        "        }\n",
        "\n",
        "    # Define K-Fold cross-validation\n",
        "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # Create GridSearchCV\n",
        "    grid = GridSearchCV(\n",
        "        estimator=model,\n",
        "        param_grid=param_grid,\n",
        "        cv=kfold,\n",
        "        scoring='neg_mean_squared_error',\n",
        "        n_jobs=-1  # Use all available cores\n",
        "    )\n",
        "\n",
        "    print(\"Starting hyperparameter tuning with 5-fold cross-validation...\")\n",
        "    print(\"This may take some time to complete.\")\n",
        "\n",
        "    # Calculate total combinations for progress reporting\n",
        "    total_combinations = len(param_grid['batch_size']) * len(param_grid['epochs'])\n",
        "\n",
        "    # Get hidden_layers param appropriately whether using scikeras or legacy\n",
        "    if 'model__hidden_layers' in param_grid:\n",
        "        total_combinations *= len(param_grid['model__hidden_layers'])\n",
        "        total_combinations *= len(param_grid['model__neurons'])\n",
        "        total_combinations *= len(param_grid['model__dropout_rate'])\n",
        "        total_combinations *= len(param_grid['model__learning_rate'])\n",
        "    else:\n",
        "        total_combinations *= len(param_grid['hidden_layers'])\n",
        "        total_combinations *= len(param_grid['neurons'])\n",
        "        total_combinations *= len(param_grid['dropout_rate'])\n",
        "        total_combinations *= len(param_grid['learning_rate'])\n",
        "\n",
        "    # Calculate total fits (combinations * CV folds)\n",
        "    total_fits = total_combinations * kfold.get_n_splits()\n",
        "\n",
        "    print(f\"\\nPerforming {total_combinations} parameter combinations with {kfold.get_n_splits()}-fold cross-validation\")\n",
        "    print(f\"Total number of model fits: {total_fits}\")\n",
        "    print(\"This process may take a while. Progress updates will be shown periodically.\")\n",
        "\n",
        "    # Set verbose to 1 to get some progress information from GridSearchCV\n",
        "    grid.verbose = 1\n",
        "\n",
        "    # Fit GridSearchCV\n",
        "    print(\"\\nStarting hyperparameter search...\")\n",
        "    start_time = time.time()\n",
        "    grid_result = grid.fit(X_train, y_train)\n",
        "    elapsed_time = time.time() - start_time\n",
        "\n",
        "    # Print timing information\n",
        "    hours, remainder = divmod(elapsed_time, 3600)\n",
        "    minutes, seconds = divmod(remainder, 60)\n",
        "    print(f\"\\nHyperparameter search completed in {int(hours):02d}:{int(minutes):02d}:{int(seconds):02d}\")\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\nBest: {-grid_result.best_score_:.2f} MSE using {grid_result.best_params_}\")\n",
        "\n",
        "    # Print all results\n",
        "    print(\"\\nGrid Search Results:\")\n",
        "    means = -grid_result.cv_results_['mean_test_score']\n",
        "    stds = grid_result.cv_results_['std_test_score']\n",
        "    params = grid_result.cv_results_['params']\n",
        "\n",
        "    # Display top 5 results\n",
        "    top_indices = means.argsort()[:5]\n",
        "    print(\"\\nTop 5 hyperparameter combinations:\")\n",
        "    for i in top_indices:\n",
        "        print(f\"MSE: {means[i]:.2f} (+/- {stds[i]:.2f}) with: {params[i]}\")\n",
        "\n",
        "    # Return best parameters in standardized format (without model__ prefix)\n",
        "    best_params = grid_result.best_params_.copy()\n",
        "\n",
        "    # For scikeras, convert model__param to param for consistency\n",
        "    if 'model__hidden_layers' in best_params:\n",
        "        best_params = {\n",
        "            'hidden_layers': best_params.get('model__hidden_layers', 2),\n",
        "            'neurons': best_params.get('model__neurons', 64),\n",
        "            'dropout_rate': best_params.get('model__dropout_rate', 0.2),\n",
        "            'learning_rate': best_params.get('model__learning_rate', 0.001),\n",
        "            'batch_size': best_params.get('batch_size', 32),\n",
        "            'epochs': best_params.get('epochs', 100)\n",
        "        }\n",
        "\n",
        "    return best_params\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "WAGZTXRugPGd"
      },
      "outputs": [],
      "source": [
        "# ## 6. Model Training and Evaluation\n",
        "#\n",
        "# This section trains the final model with the best hyperparameters and evaluates performance.\n",
        "\n",
        "def train_and_evaluate(X_train, X_test, y_train, y_test, best_params, input_dim, X_test_original=None):\n",
        "    \"\"\"Train and evaluate the neural network model with best hyperparameters\"\"\"\n",
        "    print(\"\\n===== MODEL TRAINING AND EVALUATION =====\\n\")\n",
        "\n",
        "    # Extract best parameters\n",
        "    hidden_layers = best_params.get('hidden_layers', 2)\n",
        "    neurons = best_params.get('neurons', 64)\n",
        "    dropout_rate = best_params.get('dropout_rate', 0.2)\n",
        "    learning_rate = best_params.get('learning_rate', 0.001)\n",
        "    batch_size = best_params.get('batch_size', 32)\n",
        "    epochs = best_params.get('epochs', 100)\n",
        "\n",
        "    # Create model with best parameters\n",
        "    model = create_model(\n",
        "        input_dim=input_dim,\n",
        "        hidden_layers=hidden_layers,\n",
        "        neurons=neurons,\n",
        "        dropout_rate=dropout_rate,\n",
        "        learning_rate=learning_rate\n",
        "    )\n",
        "\n",
        "    # Create callbacks\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_loss',\n",
        "        patience=20,\n",
        "        restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    model_checkpoint = ModelCheckpoint(\n",
        "        'best_model.h5',\n",
        "        save_best_only=True,\n",
        "        monitor='val_loss',\n",
        "        mode='min'\n",
        "    )\n",
        "\n",
        "    # Create progress bar callback for training\n",
        "    class TqdmProgressCallback(tf.keras.callbacks.Callback):\n",
        "        def on_epoch_begin(self, epoch, logs=None):\n",
        "            self.pbar = tqdm(total=int(self.params['steps']),\n",
        "                            desc=f\"Epoch {epoch+1}/{self.params['epochs']}\",\n",
        "                            position=0, leave=True)\n",
        "\n",
        "        def on_batch_end(self, batch, logs=None):\n",
        "            self.pbar.update(1)\n",
        "\n",
        "        def on_epoch_end(self, epoch, logs=None):\n",
        "            self.pbar.close()\n",
        "            train_loss = logs.get('loss', 0)\n",
        "            val_loss = logs.get('val_loss', 0)\n",
        "            train_mae = logs.get('mae', 0)\n",
        "            val_mae = logs.get('val_mae', 0)\n",
        "            print(f\"Epoch {epoch+1}/{self.params['epochs']} - loss: {train_loss:.4f} - val_loss: {val_loss:.4f} - mae: {train_mae:.4f} - val_mae: {val_mae:.4f}\")\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Training the final model with best hyperparameters...\")\n",
        "    history = model.fit(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        validation_split=0.2,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        verbose=0,  # Set to 0 since we're using our custom progress bar\n",
        "        callbacks=[early_stopping, model_checkpoint, TqdmProgressCallback()]\n",
        "    )\n",
        "\n",
        "    # Evaluate the model on test data with a progress bar\n",
        "    print(\"Evaluating model on test data...\")\n",
        "    y_pred = np.zeros((len(X_test), 1))\n",
        "\n",
        "    # Use batch prediction with progress bar for large datasets\n",
        "    batch_size = 32\n",
        "    num_batches = int(np.ceil(len(X_test) / batch_size))\n",
        "\n",
        "    for i in tqdm(range(num_batches), desc=\"Predicting\"):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = min((i + 1) * batch_size, len(X_test))\n",
        "        y_pred[start_idx:end_idx] = model.predict(X_test[start_idx:end_idx], verbose=0)\n",
        "\n",
        "    # Calculate performance metrics\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "    print(\"\\nModel Performance on Test Data:\")\n",
        "    print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "    print(f\"RÂ² Score: {r2:.4f}\")\n",
        "\n",
        "    # Enhanced visualization of training history\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    # Plot 1: Loss curves\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "    plt.title('Loss Curves', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('Loss (MSE)', fontsize=12)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.legend(fontsize=10)\n",
        "\n",
        "    # Plot 2: MAE curves\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.plot(history.history['mae'], label='Training MAE', color='green', linewidth=2)\n",
        "    plt.plot(history.history['val_mae'], label='Validation MAE', color='darkgreen', linewidth=2)\n",
        "    plt.title('Mean Absolute Error Curves', fontsize=14)\n",
        "    plt.xlabel('Epoch', fontsize=12)\n",
        "    plt.ylabel('MAE', fontsize=12)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.legend(fontsize=10)\n",
        "\n",
        "    # Plot 3: Actual vs Predicted values\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.scatter(y_test, y_pred, alpha=0.6, edgecolor='k')\n",
        "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2)\n",
        "    plt.xlabel('Actual Charges', fontsize=12)\n",
        "    plt.ylabel('Predicted Charges', fontsize=12)\n",
        "    plt.title('Actual vs Predicted Insurance Charges', fontsize=14)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Plot 4: Prediction Error Distribution\n",
        "    plt.subplot(2, 2, 4)\n",
        "    errors = y_test - y_pred.flatten()\n",
        "    sns.histplot(errors, kde=True)\n",
        "    plt.axvline(x=0, color='r', linestyle='--')\n",
        "    plt.title('Prediction Error Distribution', fontsize=14)\n",
        "    plt.xlabel('Prediction Error', fontsize=12)\n",
        "    plt.ylabel('Frequency', fontsize=12)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_evaluation.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "    # Additional plots for model analysis\n",
        "    plt.figure(figsize=(15, 12))\n",
        "\n",
        "    # Plot 1: Residuals vs Predicted Values\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.scatter(y_pred, errors, alpha=0.6, edgecolor='k')\n",
        "    plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "    plt.title('Residuals vs Predicted Values', fontsize=14)\n",
        "    plt.xlabel('Predicted Values', fontsize=12)\n",
        "    plt.ylabel('Residuals', fontsize=12)\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Plot 2: Prediction Error vs Age\n",
        "    if 'age' in X_test_original.columns:\n",
        "        plt.subplot(2, 2, 2)\n",
        "        plt.scatter(X_test_original['age'], errors, alpha=0.6, edgecolor='k')\n",
        "        plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "        plt.title('Prediction Error vs Age', fontsize=14)\n",
        "        plt.xlabel('Age', fontsize=12)\n",
        "        plt.ylabel('Prediction Error', fontsize=12)\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Plot 3: Prediction Error vs BMI\n",
        "    if 'bmi' in X_test_original.columns:\n",
        "        plt.subplot(2, 2, 3)\n",
        "        plt.scatter(X_test_original['bmi'], errors, alpha=0.6, edgecolor='k')\n",
        "        plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "        plt.title('Prediction Error vs BMI', fontsize=14)\n",
        "        plt.xlabel('BMI', fontsize=12)\n",
        "        plt.ylabel('Prediction Error', fontsize=12)\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Plot 4: Error Distribution by Smoker Status\n",
        "    if 'smoker' in X_test_original.columns:\n",
        "        plt.subplot(2, 2, 4)\n",
        "        sns.boxplot(x=X_test_original['smoker'], y=errors)\n",
        "        plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
        "        plt.title('Prediction Error by Smoker Status', fontsize=14)\n",
        "        plt.xlabel('Smoker', fontsize=12)\n",
        "        plt.ylabel('Prediction Error', fontsize=12)\n",
        "        plt.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('model_analysis.png', dpi=300, bbox_inches='tight')\n",
        "\n",
        "    return model, history, (mse, rmse, mae, r2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "hLAvbVi7gVN6"
      },
      "outputs": [],
      "source": [
        "# ## 7. Model Saving and Persistence\n",
        "#\n",
        "# This section implements functionality to save all model artifacts and reload them later.\n",
        "\n",
        "def save_model_artifacts(model, preprocessor, history, metrics, best_params, output_dir='model_artifacts'):\n",
        "    \"\"\"\n",
        "    Save all model artifacts including the trained model, preprocessor, history, metrics, and parameters.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : tensorflow.keras.Model\n",
        "        The trained neural network model\n",
        "    preprocessor : sklearn.compose.ColumnTransformer\n",
        "        The preprocessing pipeline used for feature transformation\n",
        "    history : tensorflow.keras.callbacks.History\n",
        "        Training history object containing loss and metrics\n",
        "    metrics : tuple\n",
        "        Tuple containing (mse, rmse, mae, r2) performance metrics\n",
        "    best_params : dict\n",
        "        Dictionary of best hyperparameters\n",
        "    output_dir : str, default='model_artifacts'\n",
        "        Directory to save model artifacts\n",
        "    \"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Generate timestamp for versioning\n",
        "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "    # Define the artifacts to save\n",
        "    artifacts = [\n",
        "        \"Model\",\n",
        "        \"Preprocessor\",\n",
        "        \"Training History\",\n",
        "        \"Performance Metrics\",\n",
        "        \"Hyperparameters\",\n",
        "        \"Manifest\"\n",
        "    ]\n",
        "\n",
        "    # Create progress bar for saving artifacts\n",
        "    with tqdm(total=len(artifacts), desc=\"Saving Model Artifacts\") as saving_pbar:\n",
        "        # 1. Save the trained model\n",
        "        model_path = os.path.join(output_dir, f\"insurance_model_{timestamp}.h5\")\n",
        "        model.save(model_path)\n",
        "        saving_pbar.update(1)\n",
        "        saving_pbar.set_description(f\"Saved {artifacts[0]}\")\n",
        "\n",
        "        # 2. Save the preprocessor\n",
        "        preprocessor_path = os.path.join(output_dir, f\"preprocessor_{timestamp}.pkl\")\n",
        "        with open(preprocessor_path, 'wb') as f:\n",
        "            pickle.dump(preprocessor, f)\n",
        "        saving_pbar.update(1)\n",
        "        saving_pbar.set_description(f\"Saved {artifacts[1]}\")\n",
        "\n",
        "        # 3. Save training history\n",
        "        history_path = os.path.join(output_dir, f\"training_history_{timestamp}.json\")\n",
        "        with open(history_path, 'w') as f:\n",
        "            history_dict = {key: [float(val) for val in values] for key, values in history.history.items()}\n",
        "            json.dump(history_dict, f)\n",
        "        saving_pbar.update(1)\n",
        "        saving_pbar.set_description(f\"Saved {artifacts[2]}\")\n",
        "\n",
        "        # 4. Save performance metrics\n",
        "        metrics_path = os.path.join(output_dir, f\"metrics_{timestamp}.json\")\n",
        "        mse, rmse, mae, r2 = metrics\n",
        "        metrics_dict = {\n",
        "            'mse': float(mse),\n",
        "            'rmse': float(rmse),\n",
        "            'mae': float(mae),\n",
        "            'r2': float(r2)\n",
        "        }\n",
        "        with open(metrics_path, 'w') as f:\n",
        "            json.dump(metrics_dict, f)\n",
        "        saving_pbar.update(1)\n",
        "        saving_pbar.set_description(f\"Saved {artifacts[3]}\")\n",
        "\n",
        "        # 5. Save hyperparameters\n",
        "        params_path = os.path.join(output_dir, f\"hyperparameters_{timestamp}.json\")\n",
        "        with open(params_path, 'w') as f:\n",
        "            json.dump(best_params, f)\n",
        "        saving_pbar.update(1)\n",
        "        saving_pbar.set_description(f\"Saved {artifacts[4]}\")\n",
        "\n",
        "        # 6. Save a manifest file with paths to all artifacts\n",
        "        manifest_path = os.path.join(output_dir, f\"manifest_{timestamp}.json\")\n",
        "        manifest = {\n",
        "            'model_path': model_path,\n",
        "            'preprocessor_path': preprocessor_path,\n",
        "            'history_path': history_path,\n",
        "            'metrics_path': metrics_path,\n",
        "            'params_path': params_path,\n",
        "            'timestamp': timestamp\n",
        "        }\n",
        "        with open(manifest_path, 'w') as f:\n",
        "            json.dump(manifest, f)\n",
        "        saving_pbar.update(1)\n",
        "        saving_pbar.set_description(f\"Saved {artifacts[5]}\")\n",
        "\n",
        "    # Print summary of saved artifacts\n",
        "    print(f\"\\nAll model artifacts saved successfully with timestamp: {timestamp}\")\n",
        "    print(f\"  - Model: {os.path.basename(model_path)}\")\n",
        "    print(f\"  - Preprocessor: {os.path.basename(preprocessor_path)}\")\n",
        "    print(f\"  - History: {os.path.basename(history_path)}\")\n",
        "    print(f\"  - Metrics: {os.path.basename(metrics_path)}\")\n",
        "    print(f\"  - Parameters: {os.path.basename(params_path)}\")\n",
        "    print(f\"  - Manifest: {os.path.basename(manifest_path)}\")\n",
        "\n",
        "    return manifest\n",
        "\n",
        "\n",
        "def load_model_artifacts(manifest_path=None, timestamp=None, output_dir='model_artifacts'):\n",
        "    \"\"\"\n",
        "    Load model artifacts from saved files.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    manifest_path : str, optional\n",
        "        Path to the manifest file. If provided, will load artifacts specified in the manifest.\n",
        "    timestamp : str, optional\n",
        "        Timestamp of the artifacts to load. If provided instead of manifest_path, will try to load\n",
        "        artifacts with this timestamp.\n",
        "    output_dir : str, default='model_artifacts'\n",
        "        Directory where model artifacts are saved\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    model : tensorflow.keras.Model\n",
        "        The loaded model\n",
        "    preprocessor : sklearn.compose.ColumnTransformer\n",
        "        The loaded preprocessor\n",
        "    metrics : dict\n",
        "        Dictionary containing the performance metrics\n",
        "    params : dict\n",
        "        Dictionary containing the hyperparameters\n",
        "    \"\"\"\n",
        "    # If manifest path is provided, load from manifest\n",
        "    if manifest_path and os.path.exists(manifest_path):\n",
        "        with open(manifest_path, 'r') as f:\n",
        "            manifest = json.load(f)\n",
        "            model_path = manifest['model_path']\n",
        "            preprocessor_path = manifest['preprocessor_path']\n",
        "            metrics_path = manifest['metrics_path']\n",
        "            params_path = manifest['params_path']\n",
        "    # If timestamp is provided, construct paths\n",
        "    elif timestamp:\n",
        "        model_path = os.path.join(output_dir, f\"insurance_model_{timestamp}.h5\")\n",
        "        preprocessor_path = os.path.join(output_dir, f\"preprocessor_{timestamp}.pkl\")\n",
        "        metrics_path = os.path.join(output_dir, f\"metrics_{timestamp}.json\")\n",
        "        params_path = os.path.join(output_dir, f\"hyperparameters_{timestamp}.json\")\n",
        "    else:\n",
        "        # Find the most recent manifest file\n",
        "        manifest_files = [f for f in os.listdir(output_dir) if f.startswith('manifest_')]\n",
        "        if not manifest_files:\n",
        "            raise FileNotFoundError(\"No manifest files found in the output directory.\")\n",
        "\n",
        "        latest_manifest = sorted(manifest_files)[-1]\n",
        "        manifest_path = os.path.join(output_dir, latest_manifest)\n",
        "\n",
        "        with open(manifest_path, 'r') as f:\n",
        "            manifest = json.load(f)\n",
        "            model_path = manifest['model_path']\n",
        "            preprocessor_path = manifest['preprocessor_path']\n",
        "            metrics_path = manifest['metrics_path']\n",
        "            params_path = manifest['params_path']\n",
        "\n",
        "    # Load model\n",
        "    print(f\"Loading model from {model_path}...\")\n",
        "    model = load_model(model_path)\n",
        "\n",
        "    # Load preprocessor\n",
        "    print(f\"Loading preprocessor from {preprocessor_path}...\")\n",
        "    with open(preprocessor_path, 'rb') as f:\n",
        "        preprocessor = pickle.load(f)\n",
        "\n",
        "    # Load metrics\n",
        "    print(f\"Loading metrics from {metrics_path}...\")\n",
        "    with open(metrics_path, 'r') as f:\n",
        "        metrics = json.load(f)\n",
        "\n",
        "    # Load hyperparameters\n",
        "    print(f\"Loading hyperparameters from {params_path}...\")\n",
        "    with open(params_path, 'r') as f:\n",
        "        params = json.load(f)\n",
        "\n",
        "    return model, preprocessor, metrics, params\n",
        "\n",
        "\n",
        "def make_prediction(model, preprocessor, new_data):\n",
        "    \"\"\"\n",
        "    Make predictions on new data using the loaded model and preprocessor.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : tensorflow.keras.Model\n",
        "        The trained model\n",
        "    preprocessor : sklearn.compose.ColumnTransformer\n",
        "        The preprocessing pipeline\n",
        "    new_data : pandas.DataFrame\n",
        "        New data to make predictions on\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    predictions : numpy.ndarray\n",
        "        Model predictions\n",
        "    \"\"\"\n",
        "    # Preprocess the new data\n",
        "    new_data_processed = preprocessor.transform(new_data)\n",
        "\n",
        "    # Make predictions\n",
        "    predictions = model.predict(new_data_processed)\n",
        "\n",
        "    return predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "373bafa3e67944fabbad8ca092a6aa7d",
            "fc80f908e985457599f99aa4bd82e653",
            "1f1d8dc440a747b0ac5a90c9390f039d",
            "f3b0c84e40114008a1253c7630017d37",
            "d31d1acba54b45f38c37ca374f1011a8",
            "eacab54397d347279e780a1babf75a6d",
            "303cc77ec1bc41e9b4c79750177623df",
            "c447c62d1c1144c2a5336983678c4522",
            "b8f6f15c08ce46f0b060139a665b2744",
            "1c0efa5ba7bb4071ad3e77dc33e1909f",
            "cfaaf166eb9a4d3bada4e4af977d469f"
          ]
        },
        "id": "2_fr7hWOP1DT",
        "outputId": "be4a71f8-32e1-4e94-db5c-ef3326522816"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "373bafa3e67944fabbad8ca092a6aa7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Project Progress:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Loading dataset...\n",
            "\n",
            "Data exploration...\n",
            "\n",
            "===== DATA EXPLORATION =====\n",
            "\n",
            "Dataset Shape: (1338, 7)\n",
            "\n",
            "Data Types:\n",
            "age           int64\n",
            "sex          object\n",
            "bmi         float64\n",
            "children      int64\n",
            "smoker       object\n",
            "region       object\n",
            "charges     float64\n",
            "dtype: object\n",
            "\n",
            "Basic Statistics:\n",
            "           count          mean           std        min         25%       50%  \\\n",
            "age       1338.0     39.207025     14.049960    18.0000    27.00000    39.000   \n",
            "bmi       1338.0     30.663397      6.098187    15.9600    26.29625    30.400   \n",
            "children  1338.0      1.094918      1.205493     0.0000     0.00000     1.000   \n",
            "charges   1338.0  13270.422265  12110.011237  1121.8739  4740.28715  9382.033   \n",
            "\n",
            "                   75%          max  \n",
            "age          51.000000     64.00000  \n",
            "bmi          34.693750     53.13000  \n",
            "children      2.000000      5.00000  \n",
            "charges   16639.912515  63770.42801  \n",
            "\n",
            "Checking for Missing Values:\n",
            "age         0\n",
            "sex         0\n",
            "bmi         0\n",
            "children    0\n",
            "smoker      0\n",
            "region      0\n",
            "charges     0\n",
            "dtype: int64\n",
            "\n",
            "Categorical Features:\n",
            "\n",
            "sex value counts:\n",
            "sex\n",
            "male      676\n",
            "female    662\n",
            "Name: count, dtype: int64\n",
            "\n",
            "smoker value counts:\n",
            "smoker\n",
            "no     1064\n",
            "yes     274\n",
            "Name: count, dtype: int64\n",
            "\n",
            "region value counts:\n",
            "region\n",
            "southeast    364\n",
            "southwest    325\n",
            "northwest    325\n",
            "northeast    324\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Checking for Outliers in Numerical Features:\n",
            "age: 0 outliers detected (0.00%)\n",
            "bmi: 9 outliers detected (0.67%)\n",
            "children: 0 outliers detected (0.00%)\n",
            "charges: 139 outliers detected (10.39%)\n",
            "\n",
            "Data visualization...\n",
            "\n",
            "===== DATA VISUALIZATION =====\n",
            "\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-32-698120641.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Data visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n{project_steps[2]}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisualize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mproject_progress\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-25-622990658.py\u001b[0m in \u001b[0;36mvisualize_data\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumerical_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatterplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'charges'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'smoker'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{col.capitalize()} vs Charges'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msubplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1562\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1563\u001b[0m         \u001b[0;31m# we have exhausted the known Axes and none match, make a new one!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1564\u001b[0;31m         \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1566\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0mprojection_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_projection_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprojection_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_axes_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, facecolor, frameon, sharex, sharey, label, xscale, yscale, box_aspect, forward_navigation_events, *args, **kwargs)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rasterization_zorder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0;31m# funcs used to format x and y - fall back on major formatters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36mclear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1450\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1452\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__clear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcla\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__clear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myaxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sharex\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mset_clip_path\u001b[0;34m(self, path, transform)\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mchild\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajorTicks\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminorTicks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1118\u001b[0;31m             \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/axis.py\u001b[0m in \u001b[0;36mset_clip_path\u001b[0;34m(self, path, transform)\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;31m# docstring inherited\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgridline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mset_clip_path\u001b[0;34m(self, path, transform)\u001b[0m\n\u001b[1;32m    813\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRectangle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m                 self.clipbox = TransformedBbox(Bbox.unit(),\n\u001b[0;32m--> 815\u001b[0;31m                                                path.get_transform())\n\u001b[0m\u001b[1;32m    816\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clippath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    817\u001b[0m                 \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/patches.py\u001b[0m in \u001b[0;36mget_transform\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;34m\"\"\"Return the `~.transforms.Transform` applied to the `Patch`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_patch_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0martist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mArtist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_data_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/patches.py\u001b[0m in \u001b[0;36mget_patch_transform\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    811\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mrotation_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrotation_point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 813\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBboxTransformTo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    814\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAffine2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mrotation_point\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mrotation_point\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36m__add__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1345\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \"\"\"\n\u001b[0;32m-> 1347\u001b[0;31m         return (composite_transform_factory(self, other)\n\u001b[0m\u001b[1;32m   1348\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTransform\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m                 NotImplemented)\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36mcomposite_transform_factory\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m   2520\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAffine2D\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAffine2D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2521\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mCompositeAffine2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2522\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mCompositeGenericTransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, a, b, **kwargs)\u001b[0m\n\u001b[1;32m   2357\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfrozen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/matplotlib/transforms.py\u001b[0m in \u001b[0;36mset_children\u001b[0;34m(self, *children)\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;31m# alive; the callback deletes the dictionary entry. This is a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0;31m# performance improvement over using WeakValueDictionary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m             ref = weakref.ref(\n\u001b[0m\u001b[1;32m    195\u001b[0m                 self, lambda _, pop=child._parents.pop, k=id_self: pop(k))\n\u001b[1;32m    196\u001b[0m             \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid_self\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mref\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Show progress bar with project stages\n",
        "    project_steps = [\n",
        "        \"Loading dataset\",\n",
        "        \"Data exploration\",\n",
        "        \"Data visualization\",\n",
        "        \"Data preprocessing\",\n",
        "        \"Hyperparameter tuning\",\n",
        "        \"Model training\",\n",
        "        \"Model evaluation\",\n",
        "        \"Saving model artifacts\"\n",
        "    ]\n",
        "\n",
        "    with tqdm(total=len(project_steps), desc=\"Project Progress\", position=0) as project_progress:\n",
        "        # Load the dataset\n",
        "        print(f\"\\n{project_steps[0]}...\")\n",
        "        df = pd.read_csv('insurance.csv')\n",
        "        project_progress.update(1)\n",
        "\n",
        "        # Data exploration\n",
        "        print(f\"\\n{project_steps[1]}...\")\n",
        "        df = explore_data(df)\n",
        "        project_progress.update(1)\n",
        "\n",
        "        # Data visualization\n",
        "        print(f\"\\n{project_steps[2]}...\")\n",
        "        df = visualize_data(df)\n",
        "        project_progress.update(1)\n",
        "\n",
        "        # Preprocess the data\n",
        "        print(f\"\\n{project_steps[3]}...\")\n",
        "        X_train_processed, X_test_processed, y_train, y_test, preprocessor, X_test_original = preprocess_data(df)\n",
        "        project_progress.update(1)\n",
        "\n",
        "    # Get number of input features after preprocessing\n",
        "    input_dim = X_train_processed.shape[1]\n",
        "\n",
        "    # Perform hyperparameter tuning\n",
        "    # Comment out for faster execution during development\n",
        "    print(f\"\\n{project_steps[4]}...\")\n",
        "    best_params = tune_hyperparameters(X_train_processed, y_train)\n",
        "    project_progress.update(1)\n",
        "\n",
        "    # If you want to skip hyperparameter tuning, uncomment and use these parameters instead\n",
        "    # best_params = {\n",
        "    #     'hidden_layers': 2,\n",
        "    #     'neurons': 64,\n",
        "    #     'dropout_rate': 0.2,\n",
        "    #     'learning_rate': 0.001,\n",
        "    #     'batch_size': 32,\n",
        "    #     'epochs': 100\n",
        "    # }\n",
        "\n",
        "    # Train and evaluate the model\n",
        "    print(f\"\\n{project_steps[5]} and {project_steps[6]}...\")\n",
        "    model, history, metrics = train_and_evaluate(\n",
        "        X_train_processed, X_test_processed, y_train, y_test, best_params, input_dim, X_test_original)\n",
        "    project_progress.update(1)  # Count both training and evaluation as one step\n",
        "\n",
        "    # Save all model artifacts\n",
        "    print(\"\\nSaving model artifacts...\")\n",
        "    with tqdm(total=5, desc=\"Saving artifacts\") as saving_progress:\n",
        "        manifest = save_model_artifacts(model, preprocessor, history, metrics, best_params)\n",
        "        saving_progress.update(5)  # Mark all saving steps complete\n",
        "    project_progress.update(1)\n",
        "\n",
        "    # Example of how to load the saved model and make predictions\n",
        "    print(\"\\n===== DEMONSTRATING MODEL LOADING =====\\n\")\n",
        "    loaded_model, loaded_preprocessor, loaded_metrics, loaded_params = load_model_artifacts(\n",
        "        manifest_path=manifest['manifest_path'])\n",
        "\n",
        "    # Create a small sample for prediction demonstration\n",
        "    sample = df.iloc[:5, :-1]  # First 5 rows, all columns except target\n",
        "    predictions = make_prediction(loaded_model, loaded_preprocessor, sample)\n",
        "\n",
        "    print(\"\\nSample Predictions:\")\n",
        "    for i, (idx, row) in enumerate(sample.iterrows()):\n",
        "        print(f\"Sample {i+1}: Age: {row['age']}, Sex: {row['sex']}, BMI: {row['bmi']:.1f}, \"\n",
        "              f\"Children: {row['children']}, Smoker: {row['smoker']}, Region: {row['region']}\")\n",
        "        print(f\"Predicted Charge: ${predictions[i][0]:.2f}, Actual Charge: ${df.iloc[i]['charges']:.2f}\\n\")\n",
        "\n",
        "    print(\"\\n===== PROJECT SUMMARY =====\\n\")\n",
        "    print(\"1. Explored the insurance dataset and identified key patterns\")\n",
        "    print(\"2. Visualized relationships between features and insurance charges\")\n",
        "    print(\"3. Preprocessed the data by encoding categorical variables and scaling numerical features\")\n",
        "    print(\"4. Performed hyperparameter tuning with cross-validation to find optimal neural network configuration\")\n",
        "    print(\"5. Trained final neural network model with early stopping to prevent overfitting\")\n",
        "    print(\"6. Evaluated model performance using multiple metrics\")\n",
        "    print(\"7. Saved all model artifacts for future use (model, preprocessor, metrics, hyperparameters)\")\n",
        "    print(\"8. Implemented functionality to load saved models and make new predictions\")\n",
        "    print(\"\\nThe model can now be used to predict insurance charges based on customer attributes.\")\n",
        "    print(\"All visualizations and model artifacts have been saved in the current directory.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1c0efa5ba7bb4071ad3e77dc33e1909f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f1d8dc440a747b0ac5a90c9390f039d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c447c62d1c1144c2a5336983678c4522",
            "max": 8,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b8f6f15c08ce46f0b060139a665b2744",
            "value": 2
          }
        },
        "303cc77ec1bc41e9b4c79750177623df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "373bafa3e67944fabbad8ca092a6aa7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc80f908e985457599f99aa4bd82e653",
              "IPY_MODEL_1f1d8dc440a747b0ac5a90c9390f039d",
              "IPY_MODEL_f3b0c84e40114008a1253c7630017d37"
            ],
            "layout": "IPY_MODEL_d31d1acba54b45f38c37ca374f1011a8"
          }
        },
        "b8f6f15c08ce46f0b060139a665b2744": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c447c62d1c1144c2a5336983678c4522": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfaaf166eb9a4d3bada4e4af977d469f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d31d1acba54b45f38c37ca374f1011a8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eacab54397d347279e780a1babf75a6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3b0c84e40114008a1253c7630017d37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c0efa5ba7bb4071ad3e77dc33e1909f",
            "placeholder": "â",
            "style": "IPY_MODEL_cfaaf166eb9a4d3bada4e4af977d469f",
            "value": "â2/8â[00:00&lt;00:02,ââ2.20it/s]"
          }
        },
        "fc80f908e985457599f99aa4bd82e653": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eacab54397d347279e780a1babf75a6d",
            "placeholder": "â",
            "style": "IPY_MODEL_303cc77ec1bc41e9b4c79750177623df",
            "value": "ProjectâProgress:ââ25%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
